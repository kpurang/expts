{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a72c471e-fe57-4c29-a6d0-8b4568aa55ed",
   "metadata": {},
   "source": [
    "# Arxiv quick survey generator\n",
    "\n",
    "Sometimes when I want to learn more about a topic by reading some papers, it is hard to find which papers are worth reading. This script uses chatgpt to help with that.\n",
    "\n",
    "The approach is simple:\n",
    "1. Given the topic of interest, query arxiv for documents.\n",
    "2. Extract the abstract and conclusion from the pdf.\n",
    "3. Send them to chat-gpt to answer some basic questions about the article\n",
    "4. Rresent the results in a table and save them in a csv file.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4b658af7-832b-43ec-a6fa-c6b7a479c868",
   "metadata": {},
   "outputs": [],
   "source": [
    "import arxiv\n",
    "import openai\n",
    "import numpy as np\n",
    "from numpy.linalg import norm\n",
    "import pypdfium2 as pdfium   # used to parse the pdf\n",
    "import re\n",
    "import time\n",
    "import os\n",
    "import pandas as pd\n",
    "from IPython.display import display\n",
    "import random\n",
    "\n",
    "axClient = arxiv.Client()\n",
    "\n",
    "EMODEL = \"text-embedding-ada-002\"  # embedding model is openai\n",
    "CMODEL = \"gpt-3.5-turbo\"  # chat model\n",
    "SEARCHMULT = 2  # multiply this by the desired number of papers for search\n",
    "PDFCACHEDIR = '/tmp/pdfs/'\n",
    "os.makedirs(PDFCACHEDIR, exist_ok=True)\n",
    "\n",
    "# Extracting conclusions and possibly abstracts are done with heuristic methods that \n",
    "# use these parameters\n",
    "MAXTITLELEN = 50 # title of conclusion can be as long as this\n",
    "MINCONCLLEN = 1000  # epect conclusions to be longer than these many chars\n",
    "TTLEXP = 100  # to verify we have a section title, lookahead and back\n",
    "\n",
    "RETRIES = 3  # rety getting papers\n",
    "\n",
    "def getPapers(query: str = 'tree search', \n",
    "              num: int = 10, \n",
    "              simThreshold: float = 0.75) -> list[dict()]:\n",
    "    \"\"\"\n",
    "    - Queries arxiv for the topic of interest\n",
    "    - finds cosine similarity between the summary and the query string\n",
    "    - returns a list of {paper, cosineSim} in order of decreasing sim\n",
    "    TBD: whether this ordering is differnet from the arxiv ordering\n",
    "    \"\"\"\n",
    "    search = arxiv.Search(\n",
    "      query = query,\n",
    "      max_results = num * SEARCHMULT,\n",
    "      sort_by = arxiv.SortCriterion.Relevance\n",
    "    )\n",
    "    results = axClient.results(search)\n",
    "    qel = openai.Embedding.create(input=query, \n",
    "                                  model=EMODEL)['data'][0]['embedding']\n",
    "    qEmb = np.array(qel)\n",
    "    qNorm = norm(qEmb)\n",
    "    docNscores = []\n",
    "    for r in results:\n",
    "        rs = {'paper': r}\n",
    "        pEmb = np.array(openai.Embedding.create(input = r.summary,\n",
    "                                         model = EMODEL)['data'][0]['embedding'])\n",
    "        rs['score'] = np.dot(pEmb, qEmb)/(qNorm * norm(pEmb))\n",
    "        if rs['score'] >= simThreshold:\n",
    "            docNscores.append(rs)\n",
    "    docNscores.sort(key=lambda x: x['score'], reverse=True)\n",
    "    return docNscores\n",
    "\n",
    "def addChunks(docNscores: list[dict()]) -> list[dict()]:\n",
    "    \"\"\"\n",
    "     Adds abstract and conclusion to the input data\n",
    "    \"\"\"\n",
    "    dsc = []\n",
    "    success = False\n",
    "    cnt = 0\n",
    "    for ds in docNscores:\n",
    "        cnt = 0\n",
    "        success = False\n",
    "        fname = os.path.join(PDFCACHEDIR, ds['paper']._get_default_filename())\n",
    "        if os.path.isfile(fname):\n",
    "            success = True\n",
    "        while not success and cnt < RETRIES:\n",
    "            cnt += 1\n",
    "            try:\n",
    "                fname = ds['paper'].download_pdf(PDFCACHEDIR)\n",
    "                success = True\n",
    "            except Exception as e:\n",
    "                print(e)\n",
    "            time.sleep(0.5 + random.random() * 2)\n",
    "        if not success:\n",
    "            print(f\"Cannot download {ds['paper'].title}\")\n",
    "            continue\n",
    "        pdf = pdfium.PdfDocument(fname)\n",
    "        # the summary is supposed to be the abstract\n",
    "        ds['abstract'] = ds['paper'].summary\n",
    "        # in case it is not present, we try to find it\n",
    "        if ds['abstract'].strip() == '':\n",
    "            abs = getAbs(pdf, False)\n",
    "            if abs == '':\n",
    "                print(f\"eliminating {ds['paper'].title} because no summary (abstract) provided or found\")\n",
    "                continue\n",
    "        concl = getConcl(pdf, False)\n",
    "        ds['conclusion'] = concl\n",
    "        if concl != '': \n",
    "            dsc.append(ds)\n",
    "        else:\n",
    "            print(f\"eliminating {ds['paper'].title} because no conclusion found\")\n",
    "    return dsc\n",
    "\n",
    "def getConcl(pdf, hailMary=False):\n",
    "    \"\"\"\n",
    "    Assume the conclusion is between \"Conclusion\" or \"Discussion\" and \"References\"\n",
    "    This is a heuristic approach to the probelm. A ML approach might get better results.\n",
    "    This works good enough for a v0.\n",
    "    \"\"\"\n",
    "    nPages = len(pdf)\n",
    "    done = False\n",
    "    pIdx = nPages - 1\n",
    "    conclusion = ''\n",
    "    startIdx = -1\n",
    "    endIdx = -1\n",
    "    re1 = re.compile('\\W*conclusion.*?\\n.*')   # and is < MAXCONCLTLEN\n",
    "    # try fo find the referenes.\n",
    "    while pIdx > -1:\n",
    "        page = pdf[pIdx].get_textpage().get_text_range()\n",
    "        lp = page.lower()\n",
    "        spx = 0\n",
    "        epx = len(lp)\n",
    "        while spx < epx:\n",
    "            if (rpos := lp[spx:epx].find('reference')) > -1:\n",
    "                if looksLikeTitle(lp, rpos, 'reference'):\n",
    "                    endIdx = rpos\n",
    "                    break\n",
    "                else: spx += rpos + 1\n",
    "            else: \n",
    "                break\n",
    "            time.sleep(1)\n",
    "        if endIdx == -1:\n",
    "            pIdx -= 1\n",
    "        else: break\n",
    "    # should look fro all instances of conclusion on each page\n",
    "    # should also look for 'discussion'\n",
    "    if (cpos := lp[:endIdx].find('conclusion')) > -1 and looksLikeTitle(lp[:endIdx], cpos, 'conclusion'):\n",
    "        # print(f\"Done conclusion {cpos}: {endIdx}\")\n",
    "        conclusion = lp[cpos:endIdx] + conclusion\n",
    "        return conclusion\n",
    "    else:\n",
    "        # it looks like the conclusion spans a page\n",
    "        conclusion = lp[:endIdx]\n",
    "        endIdx = None\n",
    "        if pIdx > 0:\n",
    "            pIdx -= 1\n",
    "            # print(f\"looking for conclusion at {pIdx}\")\n",
    "            page = pdf[pIdx].get_textpage().get_text_range()\n",
    "            lp = page.lower()\n",
    "            if (cpos := lp.find('conclusion')) > -1 and looksLikeTitle(lp[:endIdx], cpos, 'conclusion'):\n",
    "                conclusion = lp[cpos:] + conclusion\n",
    "                return conclusion\n",
    "            elif hailMary:\n",
    "                if len(conclusion) > MINCONCLLEN:\n",
    "                    # print(f\"Cant find conclusion, returning last chunk\")\n",
    "                    return conclusion\n",
    "                else: return lp + conclusion\n",
    "            else: return ''\n",
    "        else: return conclusion\n",
    "\n",
    "def getAbs(pdf, hailMary=False):\n",
    "    \"\"\"\n",
    "     THe abstract is between 'Abstract' and 'Introduction'\n",
    "    \"\"\"\n",
    "    nPages = len(pdf)\n",
    "    done = False\n",
    "    pIdx = 0\n",
    "    abs = ''\n",
    "    startIdx = -1\n",
    "    endIdx = -1\n",
    "    abstract = ''\n",
    "    re1 = re.compile('\\W*abstract\\W*\\n')\n",
    "    while not done and pIdx < nPages:\n",
    "        page = pdf[pIdx].get_textpage().get_text_range()\n",
    "        lp = page.lower()\n",
    "        if (ax := lp.find('abstract')) >= 0:\n",
    "            m1 = reAbs.match(lp[:ax])\n",
    "            startIdx = ax\n",
    "            break\n",
    "        elif len(page) <= MINPAGELEN:\n",
    "            pIdx += 1\n",
    "            continue\n",
    "        else:\n",
    "            if hailMary:\n",
    "                # print(\"cannot find absract. return the whole page\")\n",
    "                return page\n",
    "            else: return ''\n",
    "    if (ix := lp[ax:].find('introduction')) >= 0:\n",
    "        # assume intro is on same page\n",
    "        abstract += lp[startIdx:(ax + ix)]\n",
    "        return abstract\n",
    "    elif pIdx + 1 < nPages:\n",
    "        # looks like the abstract runs to the next page\n",
    "        abstract += lp[startIdx:]\n",
    "        page = pdf[pIdx + 1].get_textpage().get_text_range()\n",
    "        lp = page.lower()\n",
    "        if (ix := lp.find('introduction')) >= 0:\n",
    "            abstract += lp[:ix]\n",
    "            return abstract\n",
    "        else:\n",
    "            if hailMary:\n",
    "                # add this page in\n",
    "                abstract += lp\n",
    "            return abstract\n",
    "    return ''\n",
    "    \n",
    "\n",
    "def looksLikeTitle(page, pos, title):\n",
    "    # we might have the keyword 'conclusion' say in the text or in a title. \n",
    "    # this is a heuristic way to figure that out.\n",
    "    # the heuristic is that a title is a relatively short string on a line\n",
    "    strOI = page[max(0, pos - TTLEXP): (pos + TTLEXP)]\n",
    "    theRE = re.compile('([^\\n]*' + title + 's?[^\\n]*)', re.MULTILINE|re.DOTALL)\n",
    "    while True:\n",
    "        m1 = re.search(theRE, strOI)\n",
    "        if not m1: break\n",
    "        if len(m1[1]) <= MAXTITLELEN: return True\n",
    "        strOI = strOI[m1.end():]\n",
    "    return False\n",
    "\n",
    "\n",
    "def queryOpenai(query: list[dict] = [],\n",
    "               instr: str = 'You are a helpful assistant.',\n",
    "               temperature: float= 0.0,\n",
    "               ) -> (str, list[str]):\n",
    "    qObj = [{\"role\": \"system\", \"content\": instr}]\n",
    "    qObj.extend(query)\n",
    "    response = openai.ChatCompletion.create(\n",
    "                   model = CMODEL,\n",
    "                   messages = qObj,\n",
    "                   temperature = temperature)\n",
    "    answers = [x['message']['content'] for x in response['choices']]\n",
    "    return response, answers\n",
    "\n",
    "\n",
    "def getOneSummary(abs: str='', concl:str = '')->list[str]:\n",
    "    queryAC = [{\"role\": \"user\", \n",
    "                \"content\": f\"\"\"Given the paper abstract and conclusion below, answer the following questions. \n",
    "\n",
    "Questions:\n",
    "1. What is the paper about?\n",
    "2. What do the authors plan to show?\n",
    "3. Why is it significant?\n",
    "4. How do they intend to do it?\n",
    "5. What are the results?\n",
    "\n",
    "Abstract:\n",
    "{abs}\n",
    "\n",
    "Conclusion:\n",
    "{concl}\n",
    "\"\"\"}]\n",
    "    instr = \"You are a scientific researcher who has read thousands of papers and can accurately summarize the contents.\"\n",
    "    resp, ans = queryOpenai(queryAC, instr)\n",
    "    return ans[0].split('\\n')\n",
    "\n",
    "# the main method\n",
    "def getArxivSummaries(topic:str = 'NER with LLMs', num:int = 10, fname:str = None):\n",
    "    print('Getting references from Arxiv')\n",
    "    docs = getPapers(topic, num)\n",
    "    print('Downloading and parsing papers')\n",
    "    docs = addChunks(docs)\n",
    "    results = []\n",
    "    print('Querying chat-gpt')\n",
    "    for d in docs[:num]:\n",
    "        pinfo = [d['paper'].title, d['paper'].entry_id, d['paper'].published.strftime('%Y-%m')]\n",
    "        ans = getOneSummary(d['abstract'], d['conclusion'])\n",
    "        pinfo.extend(ans)\n",
    "        results.append(pinfo)\n",
    "    dfr = pd.DataFrame(results, \n",
    "                       columns=['title', 'url', 'date', 'about', 'aims', 'significance', 'methods', 'results'])\n",
    "    if fname is not None:\n",
    "        dfr.to_csv(fname, index=False)\n",
    "    display(dfr)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
