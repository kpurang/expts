{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kpurang/expts/blob/main/fineTuneQuantizedGemma.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Fine tuning gemma 7b\n",
        "\n",
        "This is derived [from this notebook.](https://colab.research.google.com/drive/1VoYNfYDKcKRQRor98Zbf2-9VQTtGJ24k?usp=sharing)"
      ],
      "metadata": {
        "id": "UUj27MHXd_BS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Setup\n",
        "Installing packages and mounting google drive."
      ],
      "metadata": {
        "id": "qbRfdnRpeyIu"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JMqglJkrFF0g"
      },
      "outputs": [],
      "source": [
        "!pip install -q -U bitsandbytes\n",
        "!pip install -q -U git+https://github.com/huggingface/transformers.git\n",
        "!pip install -q -U git+https://github.com/huggingface/peft.git\n",
        "!pip install -q -U git+https://github.com/huggingface/accelerate.git\n",
        "!pip install -q -U datasets\n",
        "!pip install -q -U trl\n",
        "\n",
        "%load_ext tensorboard\n",
        "\n",
        "import os\n",
        "import pandas as pd\n",
        "import torch\n",
        "import transformers\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
        "from peft import prepare_model_for_kbit_training, LoraConfig, get_peft_model\n",
        "from trl import SFTTrainer,  DataCollatorForCompletionOnlyLM\n",
        "from datasets import Dataset, load_dataset\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "os.makedirs('./logs', exist_ok=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Constants\n",
        "Defining constants in one place."
      ],
      "metadata": {
        "id": "62bgfeOQe_TJ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cf6_rFTzgLeb"
      },
      "outputs": [],
      "source": [
        "\n",
        "NEWMODELNAME = 'gemma_7b_ft'\n",
        "BASEDIR = '/content/drive/My Drive/Colab Notebooks/'\n",
        "OUTDIR = os.path.join(BASEDIR, NEWMODELNAME)\n",
        "LOSSFILE = os.path.join(BASEDIR, f\"losses/{NEWMODELNAME}.csv\")\n",
        "TRAINDATA_FILE= 'dataset/train.csv'\n",
        "VALDATA_FILE= 'dataset/val.csv'\n",
        "TEXT_FIELD='prompt'\n",
        "\n",
        "MODEL_ID = 'google/gemma-7b-it'\n",
        "# bnbConfig\n",
        "LOAD_IN_4BIT = True\n",
        "BNB_4BIT_USE_DOUBLE_QUANT = True\n",
        "BNB_4BIT_QUANT_TYPE = 'nf4'\n",
        "BNB_4BIT_COMPUTE_DTYPE = torch.bfloat16\n",
        "# LORA config\n",
        "LORA_R = 8\n",
        "LORA_ALPHA = 16\n",
        "LORA_TARGET_MODULES = ['q_proj', 'k_proj', 'v_proj', 'o_proj']\n",
        "LORA_DROPOUT = 0.05\n",
        "LORA_BIAS = 'none'\n",
        "LORA_TASK_TYPE = 'CAUSAL_LM'\n",
        "# Trainer args\n",
        "TRAIN_PER_DEVICE_TRAIN_BATCH_SIZE = 1\n",
        "TRAIN_GRADIENT_ACCUMULATION_STEPS = 4\n",
        "TRAIN_NUM_TRAIN_EPOCHS = 5\n",
        "TRAIN_WARMUP_STEPS = 2\n",
        "TRAIN_LEARNING_RATE = 2e-4\n",
        "TRAIN_FP16 = True\n",
        "TRAIN_LOGGING_STEPS=1\n",
        "TRAIN_OUTPUT_DIR = 'outputs'\n",
        "TRAIN_OPTIM = 'paged_adamw_8bit'\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3dBfxddqFsaF"
      },
      "outputs": [],
      "source": [
        "# 1. specify the quantization configuration\n",
        "# 2. get the tokenizer and quantized model\n",
        "\n",
        "def get_tokenizer_model():\n",
        "  model_id = MODEL_ID\n",
        "  bnb_config = BitsAndBytesConfig(\n",
        "        load_in_4bit = LOAD_IN_4BIT,\n",
        "        bnb_4bit_use_double_quant = BNB_4BIT_USE_DOUBLE_QUANT,\n",
        "        bnb_4bit_quant_type=BNB_4BIT_QUANT_TYPE,\n",
        "        bnb_4bit_compute_dtype = BNB_4BIT_COMPUTE_DTYPE,\n",
        "  )\n",
        "  tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
        "  model = AutoModelForCausalLM.from_pretrained(model_id,\n",
        "                                              quantization_config=bnb_config,\n",
        "                                              device_map={\"\":0}\n",
        "                                              )\n",
        "  return tokenizer, model\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oBNnie6qHKDL"
      },
      "outputs": [],
      "source": [
        "# add lora adaptors to the model\n",
        "\n",
        "def add_lora(model):\n",
        "  model.gradient_checkpointing_enable()\n",
        "  model = prepare_model_for_kbit_training(model)\n",
        "  config = LoraConfig(\n",
        "      r=LORA_R ,\n",
        "      lora_alpha=LORA_ALPHA ,\n",
        "      target_modules=LORA_TARGET_MODULES ,\n",
        "      lora_dropout=LORA_DROPOUT ,\n",
        "      bias=LORA_BIAS ,\n",
        "      task_type=LORA_TASK_TYPE\n",
        "  )\n",
        "\n",
        "  model = get_peft_model(model, config)\n",
        "  return model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rzEa0MJ0HpOr"
      },
      "outputs": [],
      "source": [
        "# load datset\n",
        "\n",
        "def genTrainPrompt(row):\n",
        "  # given a sample, this returns the training promt\n",
        "  return f\"\"\"\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "def genValPrompt(row):\n",
        "  # given a row, this returns the validating prompt\n",
        "  return f\"\"\"\n",
        "\"\"\"\n",
        "\n",
        "def get_datasets(trainFile=os.path.join(BASEDIR, TRAINDATA_FILE),\n",
        "                valFile=os.path.join(BASEDIR, VALDATA_FILE)):\n",
        "  trainDf = pd.read_csv(trainFile, index_col=0, header=0)\n",
        "  print(f\"Dataset len: {len(trainDf)}\")\n",
        "  tpDfSer = trainDf.apply(lambda x: genTrainPrompt(x), axis=1)\n",
        "  tpDf = pd.DataFrame(tpDfSer, columns=['prompt'])\n",
        "\n",
        "  valDf = pd.read_csv(valFile, index_col=0, header=0)\n",
        "  vpDfSer = valDf.apply(lambda x: genValPrompt(x), axis=1)\n",
        "  vpDf = pd.DataFrame(vpDfSer, columns=['prompt'])\n",
        "\n",
        "  trainDataset = Dataset.from_pandas(tpDf)\n",
        "  valDataset = Dataset.from_pandas(vpDf)\n",
        "  return trainDataset, valDataset\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZNI-HNZUHu38"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "def get_trainer(tokenizer, model, trainDataset, valDataset):\n",
        "  tokenizer.pad_token = tokenizer.eos_token\n",
        "  response_template = \"### PROMPT:\"\n",
        "  collator =  DataCollatorForCompletionOnlyLM(response_template, tokenizer=tokenizer)\n",
        "\n",
        "  trainer = SFTTrainer(\n",
        "      model=model,\n",
        "      train_dataset = trainDataset,\n",
        "      eval_dataset = valDataset,\n",
        "      dataset_text_field=TEXT_FIELD,\n",
        "      args = transformers.TrainingArguments(\n",
        "          per_device_train_batch_size=TRAIN_PER_DEVICE_TRAIN_BATCH_SIZE ,\n",
        "          gradient_accumulation_steps=TRAIN_GRADIENT_ACCUMULATION_STEPS ,\n",
        "          num_train_epochs = TRAIN_NUM_TRAIN_EPOCHS,\n",
        "          warmup_steps=TRAIN_WARMUP_STEPS ,\n",
        "          #max_steps=10,\n",
        "          learning_rate=TRAIN_LEARNING_RATE ,\n",
        "          fp16=TRAIN_FP16 ,\n",
        "          logging_steps=TRAIN_LOGGING_STEPS,\n",
        "          output_dir='./logs',\n",
        "          optim=TRAIN_OPTIM ,\n",
        "      ),\n",
        "      data_collator=collator,\n",
        "  )\n",
        "  model.config.use_cache = False # turn on for inference\n",
        "  return trainer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R22HgzxtI4V3"
      },
      "outputs": [],
      "source": [
        "def start_train():\n",
        "  try:\n",
        "    os.makedirs(OUTDIR, exist_ok=False)\n",
        "  except Exception as e:\n",
        "    print('Output directory may exist.\\n', e)\n",
        "  trainDataset, valDataset = get_datasets()\n",
        "  print('Done Datasets')\n",
        "  tokenizer, model = get_tokenizer_model()\n",
        "  print('Done base model')\n",
        "  model = add_lora(model)\n",
        "  print('DOne add Lora')\n",
        "  trainer = get_trainer(tokenizer, model, trainDataset, valDataset)\n",
        "  print('Done trainer')\n",
        "  trainer.train()\n",
        "  print('Done train')\n",
        "  trainer.model.save_pretrained(OUTDIR)\n",
        "  print('Done save model')\n",
        "  lossDF = pd.DataFrame(trainer.state.log_history)\n",
        "  lossDF.to_csv(LOSSFILE, index=True, header=True)\n",
        "  print('Saved losses')\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6AYDuiOujG24"
      },
      "outputs": [],
      "source": [
        "with torch.no_grad():\n",
        "  torch.cuda.empty_cache()\n",
        "%tensorboard --logdir ./logs\n",
        "start_train()\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "provenance": [],
      "authorship_tag": "ABX9TyPO6VA/pl6P4sie/y8fuU/n",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}