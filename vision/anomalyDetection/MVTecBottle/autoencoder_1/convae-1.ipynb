{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convolutional autoencoder for image anomaly detection\n",
    "\n",
    "This is about using an autoencoder to find anomalies in images. The dataset used is the MVTEC bottles dataset and the loss function is [SSIM](https://en.wikipedia.org/wiki/Structural_similarity_index_measure) This approach was prompted by [The MVTec Anomaly Detection Dataset: A Comprehensive Real-World Dataset for Unsupervised Anomaly Detection](https://link.springer.com/content/pdf/10.1007/s11263-020-01400-4.pdf)\n",
    "\n",
    "- Anomalous images: the [MVTEC](https://www.mvtec.com/company/research/datasets/mvtec-ad) bottles dataset.\n",
    "- Distance measure: Piqa [SSIM](https://piqa.readthedocs.io/en/stable/api/piqa.ssim.html)\n",
    "\n",
    "Steps:\n",
    "1. augment the data\n",
    "2. build the autoencoder\n",
    "3. train using SSIM loss\n",
    "4. test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SSIM loss\n",
    "We use piqa to provide the SSIM loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install piqa\n",
    "import piqa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np \n",
    "import pandas as pd \n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "import torchvision\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import cv2\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import shutil\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from datetime import datetime\n",
    "import math\n",
    "import piqa\n",
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.metrics import roc_auc_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Directories\n",
    "BASEDIR = \n",
    "INPUT_TRAIN_DIR = os.path.join(BASEDIR, 'mvtec-bottles/bottle/train/good/')\n",
    "TEST_DIR = os.path.join(BASEDIR, 'mvtec-bottles/bottle/test')\n",
    "AUGMENTED_DIR = os.path.join(BASEDIR, 'augmented/')\n",
    "os.makedirs(AUGMENTED_DIR, exist_ok=True)\n",
    "OUT_DIR = os.path.join(BASEDIR, 'outputs')\n",
    "os.makedirs(OUT_DIR, exist_ok=True)\n",
    "TB_DIR = os.path.join(BASEDIR, 'tb_runs')\n",
    "RESULT_DIR = os.path.join(BASEDIR, 'results')\n",
    "os.makedirs(RESULT_DIR, exist_ok=True)\n",
    "\n",
    "CPU = torch.device('cpu')\n",
    "if torch.cuda.is_available():\n",
    "    DEVICE = torch.device('cuda')\n",
    "elif torch.backends.mps.is_available():\n",
    "    DEVICE = torch.device('mps')\n",
    "else:\n",
    "    DEVICE = torch.device('cpu')\n",
    "print(DEVICE)\n",
    "\n",
    "\n",
    "RANDOM_SEED = 13\n",
    "random.seed(RANDOM_SEED)\n",
    "torch.manual_seed(RANDOM_SEED)\n",
    "\n",
    "# augmenting and resizing\n",
    "NUM_AUGMENTATIONS = 20\n",
    "NEW_IMAGE_SIZE = 256\n",
    "\n",
    "# training parameters\n",
    "BATCH_SIZE = 4\n",
    "EPOCHS = 7\n",
    "LR = 0.001\n",
    "DROPOUT = 0.2\n",
    "LOG_INTERVAL = 100\n",
    "VAL_LOG_INTERVAL = 20 \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-09T18:45:25.200362Z",
     "iopub.status.busy": "2024-08-09T18:45:25.199906Z",
     "iopub.status.idle": "2024-08-09T18:45:25.214282Z",
     "shell.execute_reply": "2024-08-09T18:45:25.213089Z",
     "shell.execute_reply.started": "2024-08-09T18:45:25.200313Z"
    }
   },
   "outputs": [],
   "source": [
    "# Utilities\n",
    "\n",
    "# diaplay a bunch of images in a grid\n",
    "\n",
    "def tile_images(imgs, rows, cols, is_paths=False):\n",
    "  \"\"\"\n",
    "  Tiles a list of images into a grid with specified rows and columns.\n",
    "\n",
    "  Args:\n",
    "      imgs: A list of torch images or of paths\n",
    "      rows: Number of rows in the grid.\n",
    "      cols: Number of columns in the grid.\n",
    "\n",
    "  Returns:\n",
    "      A matplotlib figure object containing the tiled image grid.\n",
    "  \"\"\"\n",
    "  if is_paths:\n",
    "    imgs = [Image.open(f) for f in imgs]\n",
    "    width, height = imgs[0].size()  # Assuming all images have same size\n",
    "  else:\n",
    "    imgs = [x.permute(1, 2, 0) for x in imgs]\n",
    "    width, height, _ = imgs[0].size()\n",
    "\n",
    "  # Create a new figure with a white background\n",
    "  fig, axs = plt.subplots(rows, cols, figsize=(cols * width / 100, rows * height / 100), \n",
    "                          facecolor='white')\n",
    "\n",
    "  # Iterate over images and add them to subplots\n",
    "  i = 0\n",
    "  for r in range(rows):\n",
    "    for c in range(cols):\n",
    "      if i < len(imgs):\n",
    "        axs[r, c].imshow(imgs[i])\n",
    "        axs[r, c].axis('off')  # Hide axes for cleaner visualization\n",
    "      i += 1\n",
    "\n",
    "  # Adjust layout to prevent overlapping labels (optional)\n",
    "  fig.tight_layout()\n",
    "  plt.show(fig)\n",
    "  return fig\n",
    "\n",
    "# count trainable parameters in a model\n",
    "def count_parameters(model, print_all_parms=False):\n",
    "    total_params = 0\n",
    "    if print_all_parms: print('\\nTrainable Parameters')\n",
    "    for name, parameter in model.named_parameters():\n",
    "        if not parameter.requires_grad:\n",
    "            continue\n",
    "        params = parameter.numel()\n",
    "        if print_all_parms: print(name, '\\t', params)\n",
    "        total_params += params\n",
    "    print(f\"Total Trainable Params: {total_params}\")\n",
    "    return total_params\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def view_train(n=6, rows=3, cols=2):\n",
    "    assert rows * cols == n\n",
    "    allFiles = os.listdir(INPUT_TRAIN_DIR)\n",
    "    to_view = [os.path.join(INPUT_TRAIN_DIR, random.choice(allFiles)) for i in range(n)]\n",
    "    tile_images(to_view, rows, cols, True)\n",
    "    \n",
    "#view_train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data augmentation\n",
    "\n",
    "We have 200 images which are too few, so we do some data augmentation. Bergman applies rotation, mirrorring and translation. THe bottle images seem centered and all of the same size. So we start with rotation and mirrorring. \n",
    "\n",
    "Bergman also scales the images to 256x256, so we do that too.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-11T21:37:03.680953Z",
     "iopub.status.busy": "2024-07-11T21:37:03.680519Z",
     "iopub.status.idle": "2024-07-11T21:37:03.693224Z",
     "shell.execute_reply": "2024-07-11T21:37:03.692005Z",
     "shell.execute_reply.started": "2024-07-11T21:37:03.680922Z"
    }
   },
   "outputs": [],
   "source": [
    "# Define the augmentation functions (modify as needed)\n",
    "def rotate(image, angle):\n",
    "    \"\"\"Rotates an image by a random angle.\"\"\"\n",
    "    rows, cols, _ = image.shape\n",
    "    center = (cols // 2, rows // 2)\n",
    "    rot_mat = cv2.getRotationMatrix2D(center, angle, 1.0)\n",
    "    return cv2.warpAffine(image, rot_mat, (cols, rows),\n",
    "                            borderMode=cv2.BORDER_CONSTANT,\n",
    "                            borderValue=(255,255,255))\n",
    "\n",
    "def flip(image, mode):\n",
    "    \"\"\"Flips an image horizontally or vertically.\"\"\"\n",
    "    return cv2.flip(image, mode)\n",
    "\n",
    "# Define augmentation parameters (adjust as needed)\n",
    "rotation_range = 180  # Range of degrees for random rotation\n",
    "flip_probability = 0.5  # Probability of horizontal or vertical flip\n",
    "\n",
    "def augment_images(n_augment=NUM_AUGMENTATIONS, n_files=-1):\n",
    "    cnt = 0\n",
    "    output_dir = AUGMENTED_DIR\n",
    "    for filename in os.listdir(INPUT_TRAIN_DIR):\n",
    "        if filename.endswith(\".jpg\") or filename.endswith(\".png\"):\n",
    "      # Read the image\n",
    "            if n_files > 0 and cnt > n_files: break\n",
    "            if cnt % 20 == 0:\n",
    "                print('augmented ', cnt)\n",
    "            shutil.copy(os.path.join(INPUT_TRAIN_DIR, filename),\n",
    "                        os.path.join(output_dir, filename))\n",
    "            for i in range(n_augment):\n",
    "                image = cv2.imread(os.path.join(INPUT_TRAIN_DIR, filename))\n",
    "                augmented_image = rotate(image.copy(), random.uniform(-rotation_range, rotation_range))\n",
    "                if random.random() < flip_probability:\n",
    "                    augmented_image = flip(augmented_image, random.randint(0, 1))\n",
    "                augmented_image = cv2.resize(augmented_image, (NEW_IMAGE_SIZE, NEW_IMAGE_SIZE), \n",
    "                                             interpolation=cv2.INTER_AREA)\n",
    "                # Save the augmented image with a modified filename\n",
    "                cv2.imwrite(os.path.join(output_dir, f\"aug_{i}_{filename}\"), augmented_image)\n",
    "                cnt += 1\n",
    "    return cnt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_augment():\n",
    "    augment_images(5, 1)\n",
    "    allFiles = os.listdir(AUGMENTED_DIR)\n",
    "    to_view = [os.path.join(AUGMENTED_DIR, f) for f in allFiles]\n",
    "    tile_images(to_view, 3, 2, True)\n",
    "    \n",
    "test_augment()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "augment_images()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset\n",
    "\n",
    "Assume the augmented data is split into train/val beforehand, and that the labels are available for the test case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-09T18:45:36.417978Z",
     "iopub.status.busy": "2024-08-09T18:45:36.417540Z",
     "iopub.status.idle": "2024-08-09T18:45:36.444005Z",
     "shell.execute_reply": "2024-08-09T18:45:36.442760Z",
     "shell.execute_reply.started": "2024-08-09T18:45:36.417944Z"
    }
   },
   "outputs": [],
   "source": [
    "class BottleDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Given.a list of filenames and possibly a lsot of labels, generate the data, Transforms:\n",
    "    - scale to [0,1]\n",
    "    - resize to 256x256\n",
    "    \"\"\"\n",
    "    def __init__(self, fnames, labels=None):\n",
    "        super().__init__()\n",
    "        self.fnames = fnames\n",
    "        print('dataset len fnames ', len(fnames))\n",
    "        self.labels = labels\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.fnames)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        image = torchvision.io.read_image(self.fnames[idx])\n",
    "        image = torchvision.transforms.functional.resize(image, (256, 256))\n",
    "        image = image/255\n",
    "        if image.size()[0] != 3:\n",
    "            print('ERROR in ', idx, ': ', self.fnames[idx])\n",
    "        if self.labels is None:\n",
    "            return image, None\n",
    "        else:\n",
    "            return image, self.labels[idx]\n",
    "\n",
    "def get_dataloaders(file_dir,   # all files are in this dir\n",
    "                    props=[1],  # proportions to split the data in\n",
    "                    batch_size=BATCH_SIZE,  # dataloader batch size\n",
    "                    num_samples=-1):\n",
    "    labels = None\n",
    "    file_list = os.listdir(file_dir)\n",
    "    file_list = [x for x in file_list if x.endswith('.png')]\n",
    "    print(f\"{file_dir} num pngs: {len(file_list)}\")\n",
    "    random.shuffle(file_list)\n",
    "    if num_samples > 0:\n",
    "        file_list = file_list[:num_samples]\n",
    "    print(f\"len file_list: {len(file_list)}\")\n",
    "    dls = []\n",
    "    lb, ub = 0, 0\n",
    "    for i, prop in enumerate(props):\n",
    "        if i == len(props) - 1:\n",
    "            ub = len(file_list)\n",
    "        else:\n",
    "            ub = lb + math.floor(prop * len(file_list))\n",
    "        if labels is None:\n",
    "            ds = BottleDataset([os.path.join(file_dir, x) for x in file_list[lb:ub]],\n",
    "                               [-1] * (ub-lb))\n",
    "        else:\n",
    "            ds = BottleDataset([os.path.join(file_dir, x) for x in file_list[lb:ub]], \n",
    "                               labels[lb:ub])\n",
    "        dls.append(DataLoader(ds, batch_size=batch_size, shuffle=True))\n",
    "        lb = ub\n",
    "    #for pdl in zip(props, dls):\n",
    "    #    print(f\"Prop: {pdl[0]}, num batches: {len(pdl[1])}\")\n",
    "    return dls\n",
    "\n",
    "def get_test_dataloader(test_dir, batch_size=BATCH_SIZE, num_samples=10):\n",
    "    # test dir has subdirs: broken_large, broken_small, contaminations, and good\n",
    "    dir_label = [['broken_large', 1], ['broken_small', 1], ['contamination', 1],\n",
    "                 ['good', 0]]\n",
    "    file_label = []\n",
    "    for x in dir_label:\n",
    "        xp = os.path.join(test_dir, x[0])\n",
    "        files = [os.path.join(xp, f) for f in os.listdir(xp) if f.endswith('.png')]\n",
    "        file_label.extend(list(zip(files, [x[1]] * len(files))))\n",
    "    random.shuffle(file_label)\n",
    "    print('TEST SIZE: ', len(file_label))\n",
    "    if num_samples > 0:\n",
    "        file_label = file_label[:num_samples]\n",
    "    files = [x[0] for x in file_label]\n",
    "    labels = [x[1] for x in file_label]\n",
    "    t_ds = BottleDataset(files, labels)\n",
    "    t_dl = DataLoader(t_ds, batch_size=batch_size)\n",
    "    return t_dl\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test\n",
    "\n",
    "dataset = BottleDataset(['003.png', '054.png', '143.png', '043.png'], INPUT_TRAIN_DIR, None)\n",
    "print(len(dataset))\n",
    "images = []\n",
    "for i in range(4):\n",
    "    ximg = dataset[i][0]\n",
    "    print(ximg.size())\n",
    "    images.append(ximg)\n",
    "_ = tile_images(images, 2, 2, False)\n",
    "dataset = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convolutional autoencoder\n",
    "\n",
    "Bergman does not provvide details about the autoencoder except that SSIM is used for the loss function.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-09T18:45:52.256437Z",
     "iopub.status.busy": "2024-08-09T18:45:52.255974Z",
     "iopub.status.idle": "2024-08-09T18:45:52.270131Z",
     "shell.execute_reply": "2024-08-09T18:45:52.268580Z",
     "shell.execute_reply.started": "2024-08-09T18:45:52.256403Z"
    }
   },
   "outputs": [],
   "source": [
    "class Autoencoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Autoencoder, self).__init__()\n",
    "        self.dropout = nn.Dropout(p=DROPOUT)\n",
    "        \n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Conv2d(3, 16, kernel_size=3, stride=2, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(16, 32, kernel_size=3, stride=2, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(32, 64, kernel_size=3, stride=2, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(64, 128, kernel_size=3, stride=2, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "        )\n",
    "\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.ConvTranspose2d(128, 64, kernel_size=3, stride=2, padding=1, output_padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.ConvTranspose2d(64, 32, kernel_size=3, stride=2, padding=1, output_padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.ConvTranspose2d(32, 16, kernel_size=3, stride=2, padding=1, output_padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.ConvTranspose2d(16, 3, kernel_size=3, stride=2, padding=1, output_padding=1),\n",
    "            nn.Sigmoid()  # Output between 0 and 1 for image reconstruction\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        encoded = self.encoder(x)\n",
    "        encoded = self.dropout(encoded)\n",
    "        decoded = self.decoder(encoded)\n",
    "        return decoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-10T20:36:31.107410Z",
     "iopub.status.busy": "2024-07-10T20:36:31.107018Z",
     "iopub.status.idle": "2024-07-10T20:36:31.364812Z",
     "shell.execute_reply": "2024-07-10T20:36:31.363730Z",
     "shell.execute_reply.started": "2024-07-10T20:36:31.107379Z"
    }
   },
   "outputs": [],
   "source": [
    "# test autoencoder\n",
    "\n",
    "def test_autoencoder():\n",
    "    imgs = []\n",
    "    imgs.append(torchvision.io.read_image(os.path.join(INPUT_TRAIN_DIR, '001.png')))\n",
    "    imgs.append(torchvision.io.read_image(os.path.join(INPUT_TRAIN_DIR, '002.png')))\n",
    "    imgs.append(torchvision.io.read_image(os.path.join(INPUT_TRAIN_DIR, '003.png')))\n",
    "    imgs.append(torchvision.io.read_image(os.path.join(INPUT_TRAIN_DIR, '004.png')))\n",
    "    for i in range(len(imgs)):\n",
    "        imgs[i] = torchvision.transforms.functional.resize(imgs[1], (256, 256))/255\n",
    "    p = torch.stack(imgs)\n",
    "    print('input ', p.size())\n",
    "    ae = Autoencoder()\n",
    "    ssim_loss = SSIMLoss()\n",
    "    ae.eval()\n",
    "    with torch.no_grad():\n",
    "        preds = ae(p)\n",
    "        print('preds ', preds.size())\n",
    "        loss = ssim_loss(preds, p)\n",
    "        print('loss: ', loss.item())\n",
    "        \n",
    "# test_autoencoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-09T18:21:03.998801Z",
     "iopub.status.busy": "2024-08-09T18:21:03.998093Z",
     "iopub.status.idle": "2024-08-09T18:21:04.018308Z",
     "shell.execute_reply": "2024-08-09T18:21:04.017278Z",
     "shell.execute_reply.started": "2024-08-09T18:21:03.998758Z"
    }
   },
   "outputs": [],
   "source": [
    "def train(train_file_dir = AUGMENTED_DIR, learning_rate=LR, epochs=EPOCHS, num_samples=-1):\n",
    "    # TODO load a chekpoint\n",
    "    start_time = datetime.now()\n",
    "    trained_model_name = os.path.join(OUT_DIR, f\"{start_time.strftime('%b%d_%H%M')}_model.pt\")\n",
    "    checkpoint_name = os.path.join(OUT_DIR, f\"{start_time.strftime('%b%d_%H%M')}_checkpoint.pt\")\n",
    "    tb_dir = os.path.join(TB_DIR, f\"{start_time.strftime('%b%d_%H%M')}\")\n",
    "    os.makedirs(tb_dir, exist_ok=True)\n",
    "    tb_writer = SummaryWriter(tb_dir)\n",
    "    train_val = get_dataloaders(train_file_dir, [0.85, 0.15], BATCH_SIZE, num_samples)\n",
    "    train_dl = train_val[0]\n",
    "    val_dl = train_val[1]\n",
    "    \n",
    "    model = Autoencoder().to(DEVICE)\n",
    "    count_parameters(model, True)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    ssim_loss = piqa.SSIM().to(DEVICE)\n",
    "\n",
    "    int_loss, total_loss, best_epoch, best_loss = 0, 0, -1, 1e9\n",
    "    checkpoint = None\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        int_loss, total_loss = 0, 0\n",
    "        model.train()\n",
    "        for b, data in enumerate(train_dl):\n",
    "            optimizer.zero_grad()\n",
    "            reconstructed = model(data[0].to(DEVICE))\n",
    "            loss = 1- ssim_loss(reconstructed, data[0].to(DEVICE))\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            int_loss += loss.item()\n",
    "            total_loss += loss.item()\n",
    "            \n",
    "            if b % LOG_INTERVAL == 0 and b > 0:\n",
    "                avg_loss = int_loss/LOG_INTERVAL\n",
    "                tb_writer.add_scalar('train_Loss', avg_loss, epoch * len(train_dl) + b)\n",
    "                print('train_Loss ', epoch * BATCH_SIZE + b, ': ', avg_loss)\n",
    "                int_loss = 0\n",
    "        val_int_loss, val_total_loss = 0, 0\n",
    "        avg_train_loss = total_loss/len(train_dl)\n",
    "        val_int_loss, val_total_loss, avg_val_loss = 0, 0, 0\n",
    "        model.eval()\n",
    "        print('Validating with batches: ', len(val_dl))\n",
    "        for b, data in enumerate(val_dl):\n",
    "            with torch.no_grad():\n",
    "                dataSize = data[0].size()\n",
    "                lastData = data[0][dataSize[0] - 1, :, :, :]\n",
    "                preds = model(data[0].to(DEVICE))\n",
    "                loss = 1 - ssim_loss(preds, data[0].to(DEVICE))\n",
    "                val_int_loss += loss.item()\n",
    "                val_total_loss += loss.item()\n",
    "            if b % VAL_LOG_INTERVAL == 0 and b > 0:\n",
    "                avg_val_loss = int_loss/LOG_INTERVAL\n",
    "                tb_writer.add_scalar('val_loss', avg_val_loss, epoch * len(val_dl) + b)\n",
    "                print(f\"val_loss {epoch * len(val_dl) + b}: {avg_val_loss}\")\n",
    "            data_pred = []\n",
    "            if b == len(val_dl) - 1:\n",
    "                for i in range(len(data[0])):\n",
    "                    data_pred += [data[0][i].to(CPU), preds[i].to(CPU)]\n",
    "                tile_images(data_pred, len(data[0]), 2, False)\n",
    "        avg_val_loss = val_total_loss/len(val_dl)\n",
    "        tb_writer.add_scalars(\"Epoch train val loss\", {'train_loee': avg_train_loss,\n",
    "                                                      'val_loss': avg_val_loss}, epoch)        \n",
    "        print(f\"Epoch {epoch} train/val loss: {avg_train_loss} / {avg_val_loss}\")\n",
    "        if avg_val_loss < best_loss:\n",
    "            best_loss = avg_val_loss\n",
    "            best_epoch = epoch\n",
    "            checkpoint = {'model_state_dict': model.state_dict(),\n",
    "                         'epoch': epoch,\n",
    "                         'optimizer_state_dict': optimizer.state_dict(),\n",
    "                         'loss': avg_val_loss}\n",
    "            print(f\"Saving checkpoint at epoch {epoch}\")\n",
    "            torch.save(checkpoint, checkpoint_name)\n",
    "    torch.save(checkpoint['model_state_dict'], trained_model_name)\n",
    "    print('saved model')\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train(epochs=EPOCHS, num_samples=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-07T23:08:51.519088Z",
     "iopub.status.busy": "2024-07-07T23:08:51.518607Z",
     "iopub.status.idle": "2024-07-07T23:08:52.107274Z",
     "shell.execute_reply": "2024-07-07T23:08:52.106021Z",
     "shell.execute_reply.started": "2024-07-07T23:08:51.519052Z"
    }
   },
   "outputs": [],
   "source": [
    "def quick_run_AE(n=3):\n",
    "    allFiles = os.listdir(INPUT_TRAIN_DIR)\n",
    "    test_paths = [os.path.join(INPUT_TRAIN_DIR, random.choice(allFiles)) for i in range(n)]\n",
    "    ximages = []\n",
    "    ae = Autoencoder()\n",
    "    with torch.no_grad():\n",
    "        for f in test_paths:\n",
    "            image = torchvision.io.read_image(f)\n",
    "            image = torchvision.transforms.functional.resize(image, (256, 256))\n",
    "            image = image/255\n",
    "            print(image.size())\n",
    "            ximages.append(image)\n",
    "            enc = ae.encoder(image)\n",
    "            print('enc: ', enc.size())\n",
    "            #ximages.append(enc)\n",
    "            dec = ae.decoder(enc)\n",
    "            print('dec ', dec.size())\n",
    "            ximages.append(dec * 255)\n",
    "    print(type(ximages[0]))\n",
    "    tile_images(ximages, n, 2, False)\n",
    "\n",
    "#quick_run_AE()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-09T18:49:37.312021Z",
     "iopub.status.busy": "2024-08-09T18:49:37.311622Z",
     "iopub.status.idle": "2024-08-09T18:49:37.329598Z",
     "shell.execute_reply": "2024-08-09T18:49:37.328253Z",
     "shell.execute_reply.started": "2024-08-09T18:49:37.311992Z"
    }
   },
   "outputs": [],
   "source": [
    "# test\n",
    "# the test dir has 3 bad subdirs: broken_large, broken_small and contamination\n",
    "# and one good subdir: good\n",
    "#\n",
    "# generate the ROC curve and compute the AUC\n",
    "TEST_NUM_TO_DISPLAY = 2\n",
    "\n",
    "def run_test(model, dataloader):\n",
    "    scores = []\n",
    "    labels = []\n",
    "    start_time = datetime.now().strftime('%b%d_%H%M')\n",
    "    sl_fname = os.path.join(RESULT_DIR, f\"{start_time}_pred_true.csv\")\n",
    "    roc_fname = os.path.join(RESULT_DIR, f\"{start_time}_roc.csv\")\n",
    "    #ssim = piqa.ssim.SSIM(reduction='none').to(DEVICE)\n",
    "    mseloss = torch.nn.MSELoss(reduction='none')\n",
    "    for i, batch in enumerate(dataloader):\n",
    "        print('Batch ', i)\n",
    "        with torch.no_grad():\n",
    "            reconstructed = model(batch[0].to(DEVICE))\n",
    "        #sim = ssim(reconstructed, batch[0].to(DEVICE))\n",
    "        #diff = 1 - sim\n",
    "        diff = mseloss(reconstructed, batch[0].to(DEVICE)).mean(dim=(1, 2, 3))\n",
    "        scores.extend(list(diff.detach().cpu().numpy()))\n",
    "        labels.extend(list(batch[1].cpu().numpy()))\n",
    "        idxs = list(range(len(batch[0])))\n",
    "        random.shuffle(idxs)\n",
    "        display_images = []\n",
    "        for idx in idxs[:TEST_NUM_TO_DISPLAY]:\n",
    "            display_images.extend([batch[0][idx], reconstructed[idx].detach().to(CPU)])\n",
    "        tile_images(display_images, len(display_images)//2, 2, False)\n",
    "    auc = roc_auc_score(labels, scores)\n",
    "    fpr, tpr, thresholds = roc_curve(labels, scores)\n",
    "    print('FPR ', fpr)\n",
    "    print('TPR ', tpr)\n",
    "    print('Thresholds ', thresholds)\n",
    "    print('AUC ', auc)\n",
    "    sl_df = pd.DataFrame({'labels': labels, 'scores': scores})\n",
    "    roc_df = pd.DataFrame({'fpr': fpr, 'tpr': tpr, 'thresholds': thresholds})\n",
    "    sl_df.to_csv(sl_fname, header=True, index=False)\n",
    "    roc_df.to_csv(roc_fname, header=True, index=False)\n",
    "    plt.plot(fpr, tpr, 'b', label = 'AUC = %0.2f' % auc)\n",
    "    plt.show()\n",
    "    \n",
    "\n",
    "def dp_test(model_fname, test_dir):\n",
    "    model = Autoencoder()\n",
    "    model.load_state_dict(torch.load(model_fname, map_location=DEVICE))\n",
    "    model.to(DEVICE)\n",
    "    model.eval()\n",
    "    t_dl = get_test_dataloader(test_dir, batch_size=BATCH_SIZE, num_samples=-1)\n",
    "    run_test(model, t_dl)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dp_test(saved_model, TEST_DIR)"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "datasetId": 5350331,
     "sourceId": 8899785,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30732,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
