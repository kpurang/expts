{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "aad4e639-c04b-48ed-9f46-6f3a54c4eef6",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "# Using PaLM for fact chekcing\n",
    "\n",
    "An easy way to check whether statements are true is to just ask a LLM. There are 3 outcomes: True, False, Unknown.\n",
    "\n",
    "There are 2 approaches below:\n",
    "- just ask whether the statement is true or not\n",
    "- in addition, ask it to present some evidence. There are 2 variations:\n",
    "  - simply ask for evidence\n",
    "  - ask for some number of pieces of evidence\n",
    "\n",
    "It is hoped that asking for evidence is analogous to a chain of thought approach and will result in better performance.\n",
    "\n",
    "## TLDR\n",
    "\n",
    "1. The metrics are relatively low\n",
    "2. PaLM is biased to finding that statements are true\n",
    "3. Asking for evidence for its decisions exacerbates this bias\n",
    "4. PaLM makes up sources (urls) and arguments when asked for evidence\n",
    "5. PaLM does not seem to grasp the concept of evidence for or against a statement.\n",
    "\n",
    "## Implementation\n",
    "\n",
    "PaLM has some idiosyncrasies that need to be handled.\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "17425f1e-62c8-4834-a7d5-c68f07510bee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import google.generativeai as palm\n",
    "import google.generativeai.types.safety_types as safety_types\n",
    "import os\n",
    "import time\n",
    "import json\n",
    "import pandas as pd\n",
    "import random\n",
    "import re\n",
    "import logging\n",
    "\n",
    "api_key = os.environ['PALM_API_KEY']\n",
    "palm.configure(api_key=api_key)\n",
    "MODEL = 'models/text-bison-001'\n",
    "BASEDIR = os.getcwd()\n",
    "RESULTDIR = os.path.join(BASEDIR, '../procData/factcheck/palm')\n",
    "TESTDIR = os.path.join(BASEDIR, '../procData/fakeGen/chatgpt')\n",
    "\n",
    "logging.basicConfig(filename=os.path.join(BASEDIR, 'factCheck_palm.log'), \n",
    "                    level=logging.DEBUG)\n",
    "\n",
    "# without using this, a number of requests fail\n",
    "SAFETY = [\n",
    "    {'category': safety_types.HarmCategory.HARM_CATEGORY_DEROGATORY,\n",
    "     'threshold': safety_types.HarmBlockThreshold.BLOCK_NONE,\n",
    "    },\n",
    "    {'category': safety_types.HarmCategory.HARM_CATEGORY_TOXICITY,\n",
    "     'threshold': safety_types.HarmBlockThreshold.BLOCK_NONE,\n",
    "    },\n",
    "    {'category': safety_types.HarmCategory.HARM_CATEGORY_VIOLENCE,\n",
    "     'threshold': safety_types.HarmBlockThreshold.BLOCK_NONE,\n",
    "    },\n",
    "    {'category': safety_types.HarmCategory.HARM_CATEGORY_SEXUAL,\n",
    "     'threshold': safety_types.HarmBlockThreshold.BLOCK_NONE,\n",
    "    },\n",
    "    {'category': safety_types.HarmCategory.HARM_CATEGORY_MEDICAL,\n",
    "     'threshold': safety_types.HarmBlockThreshold.BLOCK_NONE,\n",
    "    },\n",
    "    {'category': safety_types.HarmCategory.HARM_CATEGORY_DANGEROUS,\n",
    "     'threshold': safety_types.HarmBlockThreshold.BLOCK_NONE,\n",
    "    },\n",
    "]\n",
    "\n",
    "RETV = re.compile('<(.*?)>')\n",
    "RENB = re.compile('(TRUE|FALSE|UNKNOWN)')\n",
    "\n",
    "def verifyStatement(slist: list[str]):\n",
    "    \"\"\"\n",
    "    Just ask if the statement is true, false or unknown\n",
    "    \"\"\"\n",
    "    stmts = '\\n'.join([f\"Statement: {x}\" for x in slist])\n",
    "    rv = []\n",
    "    query = f\"\"\"\n",
    "Provide a list of '<TRUE>', '<FALSE>', or '<UNKNOWN>' for each statement in the list below.\n",
    "\n",
    "Statement list:\n",
    "\n",
    "{stmts}\n",
    "\"\"\"\n",
    "    try:\n",
    "        palmResp = palm.generate_text(\n",
    "            model=MODEL,\n",
    "            prompt=query,\n",
    "            temperature=0,\n",
    "            candidate_count=1,\n",
    "            safety_settings = SAFETY\n",
    "        )\n",
    "        logging.info(stmts)\n",
    "        logging.debug(str(palmResp))\n",
    "        logging.info('Result\\n' + str(palmResp.result))\n",
    "    except Exception as e:\n",
    "        print('verity statement failed\\n', e)\n",
    "        logging.error('Palm failure: ' + str(e))\n",
    "        raise e\n",
    "    if len(palmResp.candidates) == 0:\n",
    "        logging.error('No candidates from palm')\n",
    "        raise ValueError('No candidates')\n",
    "    preds = []\n",
    "    preds = list(map(lambda x: getTV(x), RETV.findall(palmResp.result)))\n",
    "    if len(preds) == 0:\n",
    "        preds = list(map(lambda x: getTV(x), RENB.findall(palmResp.result)))\n",
    "    return preds, palmResp\n",
    "\n",
    "\n",
    "def verifyStatementWithNEvidence(slist: list[str], nEvid: int = None):\n",
    "    \"\"\"\n",
    "    Ask about the truth of the staements, and provide some evidence supporting the answer\n",
    "    \"\"\"\n",
    "    stmts = '\\n'.join([f\"Statement: {x}\\n\" for x in slist])\n",
    "    query = f\"\"\"For each statment in the list below, provide: \n",
    "    1. {'A few' if nEvid is None else nEvid} pieces of evidence for it with their sources\n",
    "    2. {'A few' if nEvid is None else nEvid} pieces of evidence against it with their sources\n",
    "    3. <TRUE> if you assess the statement to be true, <FALSE> if you believe it is false, <UNKNOWN> if you cannot conclude either true or false.\n",
    "\n",
    "Statement list: \n",
    "{stmts}\n",
    "\"\"\"\n",
    "    try:\n",
    "        palmResp = palm.generate_text(\n",
    "            model=MODEL,\n",
    "            prompt=query,\n",
    "            temperature=0,\n",
    "            candidate_count=1,\n",
    "            safety_settings = SAFETY\n",
    "        )\n",
    "        logging.info(stmts)\n",
    "        logging.debug(str(palmResp))\n",
    "        logging.info('Result\\n' + str(palmResp.result))\n",
    "    except Exception as e:\n",
    "        logging.error('Palm failuer: ' + str(e))\n",
    "        raise e\n",
    "    preds = list(map(lambda x: getTV(x), RETV.findall(palmResp.result)))\n",
    "    if len(preds) == 0:\n",
    "        preds = list(map(lambda x: getTV(x), RENB.findall(palmResp.result)))\n",
    "    return preds, palmResp\n",
    "    if (len(palmResp.candidates) == 0) or (len(preds) != len(slist)):\n",
    "        logging.error(f\"Num statements: {len(slist)}, num answers: {len(preds)}\")\n",
    "        raise ValueError('Wron number of answers')\n",
    "    return preds, palmResp\n",
    "\n",
    "def getTV(stv):\n",
    "    lstv = stv.lower()\n",
    "    if lstv == 'true': return True\n",
    "    elif lstv == 'false': return False\n",
    "    else: return 'Unknown'\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db658ede-e733-40b7-b437-4304e82e200e",
   "metadata": {},
   "outputs": [],
   "source": [
    "stmts = ['David Lloyd George was the next British Prime Minister after Arthur Balfour',\n",
    "         \"Moonwalk was the name of Michael Jackson\\'s autobiography written in 1988.\",\n",
    "         \"California was the last US state to reintroduce alcohol after prohibition.\",\n",
    "         \"George Bush was the director of the CIA from 1976-81.\",\n",
    "        ]\n",
    "\n",
    "\n",
    "#tvs, resp = verifyStatement(stmts)\n",
    "tvs, resp = verifyStatementWithNEvidence(stmts, 3)\n",
    "print(resp.result, '\\n', tvs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f9f9f01-8ed9-4236-9a71-b71121a794c3",
   "metadata": {},
   "source": [
    "### Applying these to the data\n",
    "\n",
    "This needs to handle intances where PaLM does not return the correct number of results.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "c8f41432-b896-4ec6-a3a2-4f2494e83b2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "MAXSLEEP = 3\n",
    "MINSLEEP = 0\n",
    "BATCHSIZE = 4\n",
    "\n",
    "from typing import Callable\n",
    "\n",
    "def testFactCheck(inFile: str,          # json list of dicts with statement, fake_statment...\n",
    "                  selectFile: str,      # json list of True, False\n",
    "                  vmethod: Callable[list[str], list[str]],  # approach to take \n",
    "                  nEvidence: int,       # if want evidence, hot many?\n",
    "                  outFile: str,         # output\n",
    "                  logFile: str = None,  # detailed output\n",
    "                  batchSize: int = 64, \n",
    "                  startIdx: int = 0,\n",
    "                  numProc: int = -1,\n",
    "                 ):\n",
    "    \"\"\"\n",
    "    The input file is a json list of dicts that contain a true statement and a false \n",
    "    statement based on the same trivia question.\n",
    "    The select file is a list of true and false that indicates which statement to pick\n",
    "    for each question\n",
    "    If there is no select file, we verify all questions\n",
    "    \"\"\"\n",
    "    results = []\n",
    "    predVals = []\n",
    "    nb = 0\n",
    "    with open(inFile, 'r') as ix:\n",
    "        testData = json.load(ix)\n",
    "    selData = None\n",
    "    if selectFile is not None:\n",
    "        with open(selectFile, 'r') as sx:\n",
    "            selData = json.load(sx)\n",
    "    if logFile is not None:\n",
    "        log = open(logFile, 'w')\n",
    "    lb = startIdx\n",
    "    ub = 0\n",
    "    while lb < len(testData) -1:\n",
    "        ub = min(lb + batchSize, len(testData))\n",
    "        if numProc > -1:\n",
    "            ub = min(ub, (numProc + startIdx))\n",
    "            if lb >= numProc + startIdx: break\n",
    "        try:\n",
    "            batch = []\n",
    "            stv = []\n",
    "            for bx in range(lb, ub):\n",
    "                if selData is not None:\n",
    "                    batch.append(testData[bx]['statement'] if selData[bx] else testData[bx]['fake_statement'])\n",
    "                    stv.append([batch[-1], selData[bx]])\n",
    "                else:\n",
    "                    batch.append(testData[bx]['statement'])\n",
    "                    stv.append([batch[-1], True])\n",
    "                    batch.append(testData[bx]['fake_statement'])\n",
    "                    stv.append([batch[-1], False])\n",
    "            try:\n",
    "                preds, palmResp = doVerify(vmethod, nEvidence, batch)\n",
    "            except Exception as e:\n",
    "                preds = []\n",
    "                palmResp = ''\n",
    "                print(e)\n",
    "            textResp = palmResp.result\n",
    "            if len(preds) != len(batch):\n",
    "                print(f\"ERROR in batch {nb}: lenght of preds = {len(preds)}, batch = {len(batch)}. Redoing one at a time\")\n",
    "                preds = []\n",
    "                textResp = ''\n",
    "                for s in batch:\n",
    "                    try:\n",
    "                        px, ox = doVerify(vmethod, nEvidence, [s])\n",
    "                        if len(px) != 1:\n",
    "                            preds.append('Unknown')\n",
    "                        else:\n",
    "                            preds += px\n",
    "                        textResp += ox.result\n",
    "                    except Exception as e:\n",
    "                        print(e)\n",
    "                        preds.append('Unknown')\n",
    "                        textResp += '--ERROR--\\n'\n",
    "            lb = ub\n",
    "            nb += 1\n",
    "            predVals.extend(preds)\n",
    "            results.extend(stv)\n",
    "            if logFile is not None:\n",
    "                log.write('\\n'.join(batch) + '\\n')\n",
    "                log.write(f\"{textResp}\\n\\n\")\n",
    "            time.sleep(MINSLEEP + random.random() * (MAXSLEEP - MINSLEEP))\n",
    "        except Exception as e:\n",
    "            print('Error in \\n', batch, '\\n', e)\n",
    "            lb = ub\n",
    "            nb += 1\n",
    "        if nb % 10 == 0:\n",
    "            if logFile is not None: log.flush()\n",
    "            print(f\"{nb} batches done\")\n",
    "            df = pd.DataFrame(results, columns=['question', 'trueAnswer'])\n",
    "            try:\n",
    "                df['palmAnswer'] = predVals\n",
    "                df['correct?'] = (df['palmAnswer'] == df['trueAnswer'])\n",
    "            except Exception as e:\n",
    "                print(e)\n",
    "            df.to_csv(outFile, index=False, header=True)\n",
    "    if logFile is not None: log.close()\n",
    "    df = pd.DataFrame(results, columns=['question', 'trueAnswer'])\n",
    "    try:\n",
    "        df['palmAnswer'] = predVals\n",
    "        df['correct?'] = (df['palmAnswer'] == df['trueAnswer'])\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        with open(os.path.join(RESULTDIR, 'predvals_buggy'), 'w') as px:\n",
    "            json.dump(predVals, px)\n",
    "            print('Dumped predVals')\n",
    "    df.to_csv(outFile, index=False, header=True)\n",
    "    if logFile is not None:\n",
    "        log.close()\n",
    "\n",
    "def doVerify(vmethod, nEvidence, batch):\n",
    "    if vmethod.__name__ == 'vertifyStatementNEvidence':\n",
    "        preds, palmResp = vmethod(batch, nEvidence)\n",
    "    else:\n",
    "        preds, palmResp = vmethod(batch)\n",
    "    return preds, palmResp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e35993fa-b83b-4a6f-9cf7-df97c5c77f46",
   "metadata": {},
   "outputs": [],
   "source": [
    "testFactCheck(inFile = os.path.join(TESTDIR, 'tf_qa-dev.json'),\n",
    "              selectFile = os.path.join(TESTDIR, 'tf_qa-dev_select1.json'),\n",
    "              vmethod = verifyStatement,\n",
    "              nEvidence = 5,\n",
    "              outFile = os.path.join(RESULTDIR, 'triviaQA_dev_noEvidence.csv'),\n",
    "              logFile = os.path.join(RESULTDIR, 'triviaQA_dev_noEvidence.log'),\n",
    "              batchSize = 4,\n",
    "              startIdx = 0,\n",
    "              numProc = -1,\n",
    "             )\n",
    "print('all done')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0b18ac4-a2b3-4a77-94a4-7bf8af32f9e9",
   "metadata": {},
   "source": [
    "# Results\n",
    "\n",
    "Metrics are below.\n",
    "\n",
    "## Interesting examokes\n",
    "\n",
    "PaLM does not seem to have a robust notion of what evidence is and how it relates to the statement it is asked to verify. Palm tends to generate either staements without source, or urls without explanation as to why they are evidence for or against the statement. So asking PaLM for evidence to help explain its decisions is not useful.\n",
    "\n",
    "### URLs only as evidence.\n",
    "~~~\n",
    "Rudolf Hess was the last inmate of Spandau jail in Berlin\n",
    "\n",
    "1. Evidence for:\n",
    "- [https://en.wikipedia.org/wiki/Rudolf_Hess](https://en.wikipedia.org/wiki/Rudolf_Hess)\n",
    "- [https://www.britannica.com/biography/Rudolf-Hess](https://www.britannica.com/biography/Rudolf-Hess)\n",
    "\n",
    "2. Evidence against:\n",
    "- [https://en.wikipedia.org/wiki/Spandau_Prison\n",
    "(https://en.wikipedia.org/wiki/Spandau_Prison)\n",
    "\n",
    "3. <TRUE>\n",
    "~~~\n",
    "\n",
    "- https://en.wikipedia.org/wiki/Rudolf_Hess does not seen to indicate that Hess whas the last inmate.\n",
    "- https://www.britannica.com/biography/Rudolf-Hess says that he was the sole inmate from 1966, which implies he was the last one.\n",
    "- https://en.wikipedia.org/wiki/Spandau_Prison does imply that Hess was the last inmate, but it is listed as evidence against that.\n",
    "\n",
    "### Statements without source as evidence.\n",
    "\n",
    "~~~\n",
    "Treasure Island is the famous novel that Men Against the Sea and Pitcairn's Island were two sequels to.\n",
    "\n",
    "1. Evidence for:\n",
    "- Treasure Island is a famous novel.\n",
    "- Men Against the Sea and Pitcairn's Island are sequels to Treasure Island.\n",
    "\n",
    "2. Evidence against:\n",
    "- Treasure Island is not a famous novel.\n",
    "- Men Against the Sea and Pitcairn's Island are not sequels to Treasure Island.\n",
    "\n",
    "3. Conclusion: <TRUE>\n",
    "~~~\n",
    "\n",
    "Part of the evidence for consists of restating the statement. THe evidence agains is the negation of the evidence for. This pattern is common\n",
    "\n",
    "The conclusion is also incorrect.\n",
    "\n",
    "### Making up evidence\n",
    "\n",
    "~~~\n",
    "Babe Ruth was the Georgia Peach.\n",
    "\n",
    "1. Evidence for:\n",
    "    - https://en.wikipedia.org/wiki/Babe_Ruth\n",
    "    - https://www.baseball-reference.com/players/r/ruthba01.shtml\n",
    "2. Evidence against:\n",
    "    - None\n",
    "3. <TRUE>\n",
    "~~~\n",
    "\n",
    "The statement is False, and  neither of the urls presented as evidence mentions 'the georgia peach'.\n",
    "\n",
    "### Fake urls\n",
    "\n",
    "~~~\n",
    "Wendy Darling played Tinker Bell in Steven Spielberg's Hook.\n",
    "\n",
    "1. Evidence for:\n",
    "    - [IMDB](https://www.imdb.com/title/tt0103777/)\n",
    "    - [Wikipedia](https://en.wikipedia.org/wiki/Hook_(film))\n",
    "2. Evidence against:\n",
    "    - None\n",
    "3. <TRUE>\n",
    "~~~\n",
    "\n",
    "The url https://www.imdb.com/title/tt0103777 refers to \"Le batteur du boléro\" which is not related to Hook or Spielberg or Wendy Darling.\n",
    "Also, the answer is incorrect. \n",
    "\n",
    "The numbers in urls that have them seem to be incorrect. Another example: https://www.youtube.com/watch?v=--5--3--5--5\n",
    "\n",
    "_1351_\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebe46c82-aebc-4021-a9e0-c974c4ddcc12",
   "metadata": {},
   "source": [
    "# Metrics\n",
    "\n",
    "Precision, recall, F1 and the confusion matrix are computed for the 3 versions: no evidence, some evidence and more evidence.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "id": "ec6bbf9e-b0cf-46f6-b91d-b1848ce7bef6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "import numpy as np\n",
    "\n",
    "def computeMetrics(inFile):\n",
    "    df = pd.read_csv(inFile, index_col=None, dtype=str)\n",
    "    preds = df['palmAnswer'].tolist()\n",
    "    trues = df['trueAnswer'].tolist()\n",
    "    labels = [True, False, 'Unknown']\n",
    "    p,r,f,s = precision_recall_fscore_support(trues, preds, labels=labels, average=None,\n",
    "                                             zero_division=np.nan)\n",
    "    prf = np.array([p[:2], r[:2], f[:2]])\n",
    "    print('Precision, recall, F1 for True, False')\n",
    "    print(prf)\n",
    "    cm = confusion_matrix(trues, preds, labels=labels)\n",
    "    print('Confusion matrix for True, False, Unknown')\n",
    "    print(cm)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "5cbb3edb-78b5-4899-a137-28da1d21cbd4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision, recall, F1 for True, False\n",
      "[[0.61754386 0.8034188 ]\n",
      " [0.88       0.46305419]\n",
      " [0.7257732  0.5875    ]]\n",
      "Confusion matrix for True, False, Unknown\n",
      "[[176  23   1]\n",
      " [109  94   0]\n",
      " [  0   0   0]]\n"
     ]
    }
   ],
   "source": [
    "# Version not asking for evidence\n",
    "\n",
    "computeMetrics(os.path.join(RESULTDIR, 'triviaQA_dev_noEvidence.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "4dc8dc13-815c-45be-8077-7f35ba32d4df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision, recall, F1 for True, False\n",
      "[[0.54938272 0.76190476]\n",
      " [0.89       0.2364532 ]\n",
      " [0.67938931 0.36090226]]\n",
      "Confusion matrix for True, False, Unknown\n",
      "[[178  15   7]\n",
      " [146  48   9]\n",
      " [  0   0   0]]\n"
     ]
    }
   ],
   "source": [
    "# asking for evidence\n",
    "computeMetrics(os.path.join(RESULTDIR, 'triviaQA_dev_3Evidence.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "b9bb10e6-4a85-40df-af6b-ce9ba86b868a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision, recall, F1 for True, False\n",
      "[[0.54938272 0.76190476]\n",
      " [0.89       0.2364532 ]\n",
      " [0.67938931 0.36090226]]\n",
      "Confusion matrix for True, False, Unknown\n",
      "[[178  15   7]\n",
      " [146  48   9]\n",
      " [  0   0   0]]\n"
     ]
    }
   ],
   "source": [
    "# asking for more evidence\n",
    "\n",
    "computeMetrics(os.path.join(RESULTDIR, 'triviaQA_dev_5Evidence.csv'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a07ba3f-0f4b-44ef-9cf5-0cf4958c4e1d",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "True statements\n",
    "~~~\n",
    "\t\t\tno evidence\t3 evidence\t5 evidence\n",
    "precision\t0.62\t\t0.55\t\t0.55\n",
    "recall\t\t0.88\t\t0.89\t\t0.89\n",
    "f1\t\t\t0.73\t\t0.68\t\t0.68\n",
    "~~~\n",
    "\n",
    "False statements\n",
    "~~~\n",
    "\t\t\tno evidence\t3 evidence\t5 evidence\n",
    "precision\t0.80\t\t0.76\t\t0.76\n",
    "recall\t\t0.46\t\t0.24\t\t0.24\n",
    "f1\t\t\t0.59\t\t0.36\t\t0.36\n",
    "~~~\n",
    "\n",
    "- Metrics are not that great.\n",
    "- There is no difference (in counts) between 3 evidence and 5 evidence. PaLM ignores the number requested\n",
    "- asking for evidence makes PaLM assign true (and unknown to a lesser extent) to more false statements\n",
    "- it also moves some true statements assigned false to true or unknown\n",
    "\n",
    "So:\n",
    "1. palm has a bias towards finding that statements are true\n",
    "2. asking for evidence for its decision tends to increase that bias\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d877502b-a821-46ce-b285-d22099898912",
   "metadata": {},
   "source": [
    "# Examples from each bin\n",
    "\n",
    "I look at some examples of statements that are true, assigned true with no evidence and with some evidence and other combinations.\n",
    "\n",
    "Observations:\n",
    "- some of the statements are not correctly generated, especially for those in the True-False-False group\n",
    "- the bias of PaLM towards assiging statements to true is apparent.\n",
    "- "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "8dd521d9-3892-4fd6-a730-20a74eac91e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "mergedDF = None\n",
    "\n",
    "def mergeDfs(leftFile, rightFile, labels=['_left', '_right']):\n",
    "    global mergedDF\n",
    "    dfLeft = pd.read_csv(leftFile, index_col=None, dtype=str)\n",
    "    dfRight = pd.read_csv(rightFile, index_col=None, dtype=str)\n",
    "    mergedDF = pd.merge(dfLeft, dfRight, how='inner', on='question', suffixes=labels)\n",
    "\n",
    "mergeDfs(os.path.join(RESULTDIR, 'triviaQA_dev_noEvidence.csv'),\n",
    "        os.path.join(RESULTDIR, 'triviaQA_dev_3Evidence.csv'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ef52573-f3fa-42bc-b17e-1b65d87abaad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getCountAndSamples(df, vals = ['True', 'True', 'True'], nsamples=10):\n",
    "    filtDf = df[(df['trueAnswer_right'] == vals[0]) & (df['palmAnswer_left'] == vals[1]) \\\n",
    "        & (df['palmAnswer_right'] == vals[2])]\n",
    "    print(f\"Number of {vals} = {len(filtDf)}\")\n",
    "    ns = min(nsamples, len(filtDf))\n",
    "    sample = filtDf.sample(n=ns)\n",
    "    for i, r in sample.iterrows():\n",
    "        print(f\"{i}: {r['question']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfe5a9f9-53de-4534-8b4d-200857b93058",
   "metadata": {},
   "outputs": [],
   "source": [
    "getCountAndSamples(mergedDF, ['True', 'True', 'True'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71f6111b-cb13-487c-954d-b756e0f22f4c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "getCountAndSamples(mergedDF, ['True', 'True', 'False'])\n",
    "\n",
    "# etd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "441035e1-cfd6-41cc-998e-57db35eec2ba",
   "metadata": {},
   "source": [
    "# Inconsistent responses\n",
    "\n",
    "In the above, PaLM was presented with either a true statement or a false statement \n",
    "of the same form as the true statement. Here, it is fed both the true statement and\n",
    "the false statement. Just one of the pairs can be true.  The experiment here is to \n",
    "find if PaLM does that.\n",
    "\n",
    "The true and false sentences for each pair are presented sequentially, which may make\n",
    "it easier for PaLM.  The order of all the statements can be randomized later if need be.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01b0a115-5fa5-4ad6-b162-3f839e0322e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "testFactCheck(inFile = os.path.join(TESTDIR, 'tf_qa-dev.json'),\n",
    "              selectFile = None,   # no select file -> verify hoth true and fake stmts\n",
    "              vmethod = verifyStatement,  # asking for evidence does not help\n",
    "              nEvidence = 5,\n",
    "              outFile = os.path.join(RESULTDIR, 'triviaQA_TandF.csv'),\n",
    "              logFile = os.path.join(RESULTDIR, 'triviaQA_TandF.log'),\n",
    "              batchSize = 2,\n",
    "              startIdx = 0,\n",
    "              numProc = -1,\n",
    "             )\n",
    "print('all done')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "254706c0-631b-406e-80a0-ef308ecda9da",
   "metadata": {},
   "source": [
    "## Computing results\n",
    "\n",
    "Given the resuts from above:\n",
    "- make each pair of rows into one row so that each row has one true and one false statement on the same topic\n",
    "- label each row as:\n",
    "  - True if the PaLM response is correct\n",
    "  - False if the response is reverse comapred to the true statements\n",
    "  - Inconsistent if both responses are true\n",
    "  - Unknown if both responses are false\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "31f53965-b8ef-4b3b-a1e5-1d230b9b136b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True:  293\n",
      "False:  62\n",
      "Inconsistent:  16\n",
      "Unknown:  31\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def evalPairs(inFile, outFile):\n",
    "    dfr = pd.read_csv(inFile, index_col=None, dtype=str)\n",
    "    df_t = dfr.iloc[::2, :]\n",
    "    nameMap = {x: f\"{x}_f\" for x in dfr.columns}\n",
    "    df_f = dfr.drop(df_t.index).reset_index(drop=True).rename(columns=nameMap)\n",
    "    df_t = df_t.reset_index(drop=True)\n",
    "    df_tf = pd.concat([df_t, df_f], axis=1, ignore_index=False)\n",
    "    numBadRows = len(df_tf[~(df_tf['trueAnswer'] == 'True')]) + len(df_tf[(df_tf['trueAnswer_f'] == 'True')])\n",
    "    if numBadRows > 0:\n",
    "        print('rows mixed up, num bad rows: ', numBadRows)\n",
    "        return\n",
    "    df_tf['pstatus'] = df_tf.apply(lambda r: getRowStatus(r), axis=1)\n",
    "    print('True: ', len(df_tf[df_tf['pstatus'] == 'True']))\n",
    "    print('False: ', len(df_tf[df_tf['pstatus'] == 'False']))\n",
    "    print('Inconsistent: ', len(df_tf[df_tf['pstatus'] == 'Inconsistent']))\n",
    "    print('Unknown: ', len(df_tf[df_tf['pstatus'] == 'Unknown']))\n",
    "    df_tf.to_csv(outFile, index=False) \n",
    "\n",
    "def getRowStatus(r):\n",
    "    if r['palmAnswer_f'] == 'False' and (r['palmAnswer'] == 'True'):\n",
    "        return 'True'\n",
    "    elif r['palmAnswer_f'] == 'True' and (r['palmAnswer'] == 'False'):\n",
    "        return 'False'\n",
    "    elif r['palmAnswer_f'] == 'True' and (r['palmAnswer'] == 'True'):\n",
    "        return 'Inconsistent'\n",
    "    else:\n",
    "        return 'Unknown'\n",
    "\n",
    "evalPairs(os.path.join(RESULTDIR, 'triviaQA_TandF.csv'),\n",
    "          os.path.join(RESULTDIR, 'triviaQA_Pairs.csv'))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d67ea0fe-536e-4627-95af-9bee74bd1e22",
   "metadata": {},
   "source": [
    "These results are better than the results obtained earlier. THe earlier computations were repeated multiple times with the same results.  \n",
    "\n",
    "Rerun the no-evidence fact check to see if the performance matches this one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02e364ed-1aff-4af0-9424-b379a613e891",
   "metadata": {},
   "outputs": [],
   "source": [
    "testFactCheck(inFile = os.path.join(TESTDIR, 'tf_qa-dev.json'),\n",
    "              selectFile = os.path.join(TESTDIR, 'tf_qa-dev_select1.json'),\n",
    "              vmethod = verifyStatement,\n",
    "              nEvidence = 5,\n",
    "              outFile = os.path.join(RESULTDIR, 'triviaQA_dev_noEvidence_Q2.csv'),\n",
    "              logFile = os.path.join(RESULTDIR, 'triviaQA_dev_noEvidence_Q2.log'),\n",
    "              batchSize = 4,\n",
    "              startIdx = 0,\n",
    "              numProc = -1,\n",
    "             )\n",
    "print('all done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "id": "9dc30834-15db-4f3f-a829-b3c09c147e2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision, recall, F1 for True, False\n",
      "[[0.60526316 0.72592593]\n",
      " [0.805      0.48275862]\n",
      " [0.69098712 0.57988166]]\n",
      "Confusion matrix for True, False, Unknown\n",
      "[[161  37   2]\n",
      " [105  98   0]\n",
      " [  0   0   0]]\n"
     ]
    }
   ],
   "source": [
    "# Version not asking for evidence. run a few days later\n",
    "\n",
    "computeMetrics(os.path.join(RESULTDIR, 'triviaQA_dev_noEvidence_Q2.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9aa30ca5-0ab1-4b72-99cf-4cc43166d446",
   "metadata": {},
   "outputs": [],
   "source": [
    "The previous metrics were better.  There seems to be instability in the results."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbb59e1a-bd80-4555-9092-5ed50bdce043",
   "metadata": {},
   "source": [
    "## Conclusions\n",
    "\n",
    "Factchecking is meant to be a way to verify whether a statement is true. In addition providing evidence is meant to help people gain confidence in the judgement of the system.\n",
    "\n",
    "1. the metrics are relatively low\n",
    "2. PaLM is biased to finding that statements are true\n",
    "3. Asking for evidence for its decisions exacerbates this bias\n",
    "4. PaLM makes up sources (urls) and arguments when asked for evidence\n",
    "5. PaLM does not seem to grasp the concept of evidence for or against a statement.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
