{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "162e6eff-d57b-4893-9da2-b44d249b3e86",
   "metadata": {},
   "source": [
    "# Using chatGPT for factchekcing\n",
    "\n",
    "An easy way to check whether statements are true is to just ask a LLM. This is a different task from asking a question where it can make things up.  In this case, there are 3 outcomes: True, False, Unknown.\n",
    "\n",
    "I take 2 approaches:\n",
    "- just ask whether the statement is true or not\n",
    "- in addition, ask it to present some evidence. There are 2 variations:\n",
    "  - simply ask for evidence\n",
    "  - ask for some number of pieces of evidence\n",
    "\n",
    "Asking for evidence is analogous to a chain of thought approach and it will be intetersting to see if this changes the performance.\n",
    "\n",
    "\n",
    "## Implementation\n",
    "\n",
    "Querying chatGPT is straightforward. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "17425f1e-62c8-4834-a7d5-c68f07510bee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "import os\n",
    "import time\n",
    "import json\n",
    "import pandas as pd\n",
    "import random\n",
    "import re\n",
    "\n",
    "BASEDIR = os.getcwd()\n",
    "TESTDIR = os.path.join(BASEDIR, 'testData')\n",
    "RESULTDIR = os.path.join(BASEDIR, 'testResults/chatgpt')\n",
    "\n",
    "MODEL = \"gpt-3.5-turbo\"\n",
    "\n",
    "reTV = re.compile('<(.*?)>')\n",
    "\n",
    "\n",
    "def verifyStatement(stmts: str):\n",
    "    nqStmts = stmts.replace(\"'\", '')\n",
    "    rv = []\n",
    "    query = [{\"role\": \"system\",\n",
    "              \"content\": \"You answer questions tersely.\"},\n",
    "            {\"role\": \"user\", \"content\": f\"\"\"For each statement in the list below, determine whether it is true or not.\n",
    "            If it is true, respond with '<TRUE>', if it is false respond with '<FALSE>', otherwise, respond with '<UNKNOWN>'. Return your responses in a json list.\n",
    "            \n",
    "            Statements: {nqStmts}\n",
    "            Answer: \"\"\"}]\n",
    "    oaiResp = openai.ChatCompletion.create(\n",
    "        model = MODEL, \n",
    "        messages = query,\n",
    "        temperature = 0)\n",
    "    response = oaiResp['choices'][0]['message']['content']\n",
    "    tvmap = {'<TRUE>': True, '<FALSE>': False, '<UNKNOWN>': 'Unknown'}\n",
    "    try:\n",
    "        respL = json.loads(response)\n",
    "    except:\n",
    "        if type(response) == list:\n",
    "            respL = response\n",
    "        else:\n",
    "            respL = [response]\n",
    "    rv = [tvmap[x] if x in tvmap else 'Unknown' for x in respL]\n",
    "    return rv, oaiResp\n",
    "\n",
    "\n",
    "def verifyStatementWithEvidence(stmts: str):\n",
    "    query = [\n",
    "        {\"role\": \"system\",\n",
    "         \"content\": \"You are a knowledgeable and conscientious agent who follows instructions exactly all the time.\"},\n",
    "        {\"role\": \"user\", \n",
    "         \"content\": f\"\"\"For each statment in the list below, provide: \n",
    "    1. A few pieces of evidence for it with their sources\n",
    "    2. A few pieces of evidence against it with their sources\n",
    "    3. <TRUE> if you assess the statement to be true, <FALSE> if you believe it is false, <UNKNOWN> if you cannot conclude either true or false.\n",
    "\n",
    "Statement list: \n",
    "{stmts}\n",
    "\"\"\"}\n",
    "    ]\n",
    "    oaiResp = openai.ChatCompletion.create(\n",
    "        model=MODEL,\n",
    "        messages=query,\n",
    "        temperature=0,\n",
    "    )\n",
    "    result = oaiResp['choices'][0]['message']['content']\n",
    "    lb = 0\n",
    "    tvs = []\n",
    "    responses = reTV.findall(result)\n",
    "    for tv in responses:\n",
    "        if tv == 'TRUE': tvs.append(True)\n",
    "        elif tv == \"FALSE\": tvs.append(False)\n",
    "        else: tvs.append('Unknown')\n",
    "    return tvs, oaiResp\n",
    "\n",
    "def verifyStatementWith4Pieces(stmts: str):\n",
    "    query = [\n",
    "        {\"role\": \"system\",\n",
    "         \"content\": \"You are a knowledgeable and conscientious agent who follows instructions exactly all the time.\"},\n",
    "        {\"role\": \"user\", \n",
    "         \"content\": f\"\"\"For each statment in the list below, provide: \n",
    "    1. 4 pieces of evidence for it, with their sources\n",
    "    2. 4 pieces of evidence against it, with their sources\n",
    "    3. <TRUE> if you assess the statement to be true, <FALSE> if you believe it is false, <UNKNOWN> if you cannot conclude either true or false.\n",
    "\n",
    "Statement list: \n",
    "{stmts}\n",
    "\"\"\"}\n",
    "    ]\n",
    "    oaiResp = openai.ChatCompletion.create(\n",
    "        model=MODEL,\n",
    "        messages=query,\n",
    "        temperature=0,\n",
    "    )\n",
    "    result = oaiResp['choices'][0]['message']['content']\n",
    "    lb = 0\n",
    "    tvs = []\n",
    "    responses = reTV.findall(result)\n",
    "    for tv in responses:\n",
    "        if tv == 'TRUE': tvs.append(True)\n",
    "        elif tv == \"FALSE\": tvs.append(False)\n",
    "        else: tvs.append('Unknown')\n",
    "    return tvs, oaiResp\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f9f9f01-8ed9-4236-9a71-b71121a794c3",
   "metadata": {},
   "source": [
    "### Driver\n",
    "\n",
    "This needs to handle intances where chatGPT does not return the correct number of results.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "c8f41432-b896-4ec6-a3a2-4f2494e83b2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "MAXSLEEP = 3\n",
    "MINSLEEP = 0\n",
    "BATCHSIZE = 4\n",
    "\n",
    "from typing import Callable\n",
    "\n",
    "def testFactCheck(inFile: str,          # json list of dicts with statement, fake_statment...\n",
    "                  selectFile: str,      # json list of True, False\n",
    "                  vmethod: Callable[list[str], list[str]],  # approach to take \n",
    "                  outFile: str,         # output\n",
    "                  logFile: str = None,  # detailed output\n",
    "                  batchSize: int = 64,  # batch size\n",
    "                  startIdx: int = 0,\n",
    "                  numProc: int = -1,\n",
    "                 ):\n",
    "    results = []\n",
    "    predVals = []\n",
    "    nb = 0\n",
    "    with open(inFile, 'r') as ix:\n",
    "        testData = json.load(ix)\n",
    "    with open(selectFile, 'r') as sx:\n",
    "        selData = json.load(sx)\n",
    "    if logFile is not None:\n",
    "        log = open(logFile, 'w')\n",
    "    lb = 0\n",
    "    ub = 0\n",
    "    while lb < len(testData) -1:\n",
    "        ub = min(lb + batchSize, len(testData))\n",
    "        if numProc > -1:\n",
    "            ub = min(ub, (numProc + startIdx))\n",
    "            if lb >= numProc + startIdx: break\n",
    "        try:\n",
    "            batch = []\n",
    "            stv = []\n",
    "            for bx in range(lb, ub):\n",
    "                batch.append(testData[bx]['statement'] if selData[bx] else testData[bx]['fake_statement'])\n",
    "                stv.append([batch[-1], selData[bx]])\n",
    "\n",
    "            preds, oaiResp = vmethod(('\\n'.join(batch)).replace(\"'\", ' '))\n",
    "            textResp = oaiResp['choices'][0]['message']['content']\n",
    "            if len(preds) != len(batch):\n",
    "                print(f\"ERROR in batch {nb}: lenght of preds = {len(preds)}, batch = {len(batch)}. Redoing one at a time\")\n",
    "                preds = []\n",
    "                textResp = ''\n",
    "                for s in batch:\n",
    "                    px, ox = vmethod(s + '\\n')\n",
    "                    if len(px) != 1:\n",
    "                        preds.append('Unknown')\n",
    "                    else:\n",
    "                        preds.append(px[0])\n",
    "                    textResp += ox['choices'][0]['message']['content']\n",
    "            lb = ub\n",
    "            nb += 1\n",
    "            predVals.extend(preds)\n",
    "            results.extend(stv)\n",
    "            if logFile is not None:\n",
    "                log.write(f\"{textResp}\\n\\n\")\n",
    "            time.sleep(MINSLEEP + random.random() * (MAXSLEEP - MINSLEEP))\n",
    "        except Exception as e:\n",
    "            print('Error in \\n', batch, '\\n', e)\n",
    "        if nb % 10 == 0:\n",
    "            log.flush()\n",
    "            print(f\"{nb} batches done\")\n",
    "            df = pd.DataFrame(results, columns=['question', 'trueAnswer'])\n",
    "            try:\n",
    "                df['gptAnswer'] = predVals\n",
    "                df['correct?'] = (df['gptAnswer'] == df['trueAnswer'])\n",
    "            except Exception as e:\n",
    "                print(e)\n",
    "            df.to_csv(outFile, index=False, header=True)\n",
    "    if logFile is not None: log.close()\n",
    "    df = pd.DataFrame(results, columns=['question', 'trueAnswer'])\n",
    "    try:\n",
    "        df['gptAnswer'] = predVals\n",
    "        df['correct?'] = (df['gptAnswer'] == df['trueAnswer'])\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        with open(os.path.join(RESULTDIR, 'predvals_buggy'), 'w') as px:\n",
    "            json.dump(predVals, px)\n",
    "            print('Dumped predVals')\n",
    "    df.to_csv(outFile, index=False, header=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e35993fa-b83b-4a6f-9cf7-df97c5c77f46",
   "metadata": {},
   "outputs": [],
   "source": [
    "testFactCheck(os.path.join(TESTDIR, 'tf_qa-dev.json'),\n",
    "              os.path.join(TESTDIR, 'tf_qa-dev_select1.json'),\n",
    "              verifyStatementWith4Pieces,\n",
    "              os.path.join(RESULTDIR, 'triviaQA_dev_4Pieces.csv'),\n",
    "              os.path.join(RESULTDIR, 'triviaQA_dev_4Pieces.log'),\n",
    "              BATCHSIZE,\n",
    "             )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5d62da7-19ca-4a35-bfec-6cc9ab4b8b6e",
   "metadata": {},
   "source": [
    "## Results\n",
    "\n",
    "Results are obtained from the same dataset as for the websearch version - a list of randomly picked true (according to triviaQA) and false statements generated by chatGPT.\n",
    "\n",
    "### Examples\n",
    "\n",
    "\n",
    "#### Evidence\n",
    "\n",
    "True statement: \n",
    ">Daniel Barenboim was married to one of the subjects of the film Hillary and Jackie.\n",
    "\n",
    "- chatGPT without evidence: Unknown\n",
    "- chatGPT with evidence: True\n",
    "\n",
    "> 1. Evidence for the statement:\n",
    ">   - Daniel Barenboim was indeed married to one of the subjects of the film \"Hillary and Jackie.\" The film is a biographical drama based on the lives of the British cellist Jacqueline du Pré and her sister Hilary du Pré. Jacqueline du Pré was married to Daniel Barenboim from 1967 until her death in 1987. This information can be verified through various sources, including:\n",
    "     - The Guardian: \"Daniel Barenboim: 'I have never been a fanatic about music'\"\n",
    "     - The New York Times: \"Daniel Barenboim: A Life in Music\"\n",
    "\n",
    ">2. Evidence against the statement:\n",
    ">   - There is no evidence against the statement, as it is accurate that Daniel Barenboim was married to one of the subjects of the film \"Hillary and Jackie.\"\n",
    "\n",
    ">3. Assessment of the truth of the statement: True\n",
    "\n",
    "chatGPT generates succinct arguments if available, together with sources. \n",
    "\n",
    "#### Evidence confusion\n",
    "\n",
    "In this case the same arguments are taken to be for and against the statement:\n",
    "\n",
    ">1. Evidence for the statement \"Niamey is the capital of Nigeria\":\n",
    "\n",
    ">- Niamey is indeed the capital of Niger, not Nigeria. This is a well-known fact and can be verified through various reputable sources such as Encyclopedia Britannica (Source: Encyclopedia Britannica - Niger).\n",
    ">- Abuja is the capital of Nigeria, as stated by the Nigerian government and recognized internationally. This information can be confirmed through official government websites, such as the Federal Republic of Nigeria's official website (Source: Official website of the Federal Republic of Nigeria).\n",
    ">- The United Nations, as well as other international organizations, recognize Abuja as the capital of Nigeria. This can be verified through the United Nations' official website or other international governmental organizations (Source: United Nations - Member States).\n",
    ">- Numerous travel guides and reputable travel websites, such as Lonely Planet and TripAdvisor, state that Abuja is the capital of Nigeria (Source: Lonely Planet - Nigeria, TripAdvisor - Nigeria).\n",
    "\n",
    ">2. Evidence against the statement \"Niamey is the capital of Nigeria\":\n",
    "\n",
    ">- Niamey is the capital of Niger, not Nigeria. This is a well-known fact and can be verified through various reputable sources such as Encyclopedia Britannica (Source: Encyclopedia Britannica - Niger).\n",
    ">- Abuja is the capital of Nigeria, as stated by the Nigerian government and recognized internationally. This information can be confirmed through official government websites, such as the Federal Republic of Nigeria's official website (Source: Official website of the Federal Republic of Nigeria).\n",
    ">- The United Nations, as well as other international organizations, recognize Abuja as the capital of Nigeria. This can be verified through the United Nations' official website or other international governmental organizations (Source: United Nations - Member States).\n",
    ">- Numerous travel guides and reputable travel websites, such as Lonely Planet and TripAdvisor, state that Abuja is the capital of Nigeria (Source: Lonely Planet - Nigeria, TripAdvisor - Nigeria).\n",
    "\n",
    ">3. <FALSE> The statement \"Niamey is the capital of Nigeria\" is false.\n",
    "\n",
    "#### Evidence may be made up\n",
    "\n",
    "False statement:\n",
    "> Uranus has moons called Proteus and Nereid.\n",
    " \n",
    ">1. Evidence for the statement:\n",
    ">   - Proteus: Proteus is indeed one of the moons of Uranus. It was discovered by Voyager 2 in 1989. Source: NASA - Solar System Exploration website (https://solarsystem.nasa.gov/moons/uranus-moons/proteus/in-depth/)\n",
    ">   - Nereid: Nereid is also a moon of Uranus. It was discovered in 1949 by Gerard Kuiper. Source: NASA - Solar System Exploration website (https://solarsystem.nasa.gov/moons/uranus-moons/nereid/in-depth/)\n",
    "\n",
    ">2. Evidence against the statement:\n",
    ">   - There is no evidence against the statement. The statement is accurate, and both Proteus and Nereid are moons of Uranus.\n",
    "\n",
    "The statement is false. While Voayger 2 did discover Proteus but not as a moon of Uranus.  The statement is assumed and stated, then a true fact is added as though providing evidence for the false statement.\n",
    "\n",
    "#### More evidence can help\n",
    "\n",
    "For the same statement, asking for more evidence gives:\n",
    "\n",
    "> Uranus has moons called Proteus and Nereid.\n",
    "\n",
    ">1. Evidence for:\n",
    "   a. Uranus has a moon named Proteus, which was discovered by Voyager 2 in 1989. (Source: NASA - Uranus Moons Fact Sheet)\n",
    "   b. Proteus is the second-largest moon of Uranus and has a irregular shape. (Source: NASA - Uranus Moons Fact Sheet)\n",
    "   c. Nereid is another moon of Uranus, discovered in 1949. (Source: NASA - Uranus Moons Fact Sheet)\n",
    "   d. Nereid is one of the outermost moons of Uranus and has an eccentric orbit. (Source: NASA - Uranus Moons Fact Sheet)\n",
    "\n",
    ">2. Evidence against:\n",
    "   a. Uranus has a total of 27 known moons, and Proteus and Nereid are not among them. (Source: NASA - Uranus Moons Fact Sheet)\n",
    "   b. The known moons of Uranus include names like Miranda, Ariel, Umbriel, Titania, and Oberon, but not Proteus or Nereid. (Source: NASA - Uranus Moons Fact Sheet)\n",
    "   c. Proteus and Nereid are moons of Neptune, not Uranus. (Source: NASA - Neptune Moons Fact Sheet)\n",
    "   d. The confusion between Uranus and Neptune moons might arise due to their similar blue color and their positions in the outer regions of the solar system.\n",
    "\n",
    ">3. <FALSE> Uranus does not have moons called Proteus and Nereid. These moons are actually associated with the planet Neptune.\n",
    "\n",
    "The evidence for the statement is still made up, but it now gets evidence against and finds that these are moons of Neptune, not Uranus.\n",
    "\n",
    "#### Made up sources\n",
    "\n",
    "In the above examples, chatGPT was  attributing false statements to specific sources.  Another issue is that most of the urls it provides as source give 404 errors. It is unlikely that most of the urls changed since they were scraped.  The more likely explanation is that chatGPT is making urls up. For example:\n",
    "\n",
    "False statement:\n",
    "> Thriller by Michael Jackson became the first ever music video to be shown on MTV.\n",
    "\n",
    "> Evidence for:\n",
    "> 1. In an article by Billboard, it is stated that \"Thriller\" was indeed the first music video to be shown on MTV. (Source: https://www.billboard.com/articles/news/467705/michael-jacksons-thriller-30-years-later-10-key-moments)\n",
    "> 2. The official MTV website also confirms that \"Thriller\" was the first music video to be aired on the channel. (Source: http://www.mtv.com/news/2697375/michael-jackson-thriller-video/)\n",
    "> 3. The Guinness World Records website recognizes \"Thriller\" as the first music video to be shown on MTV. (Source: https://www.guinnessworldrecords.com/world-records/first-music-video-shown-on-mtv)\n",
    "> 4. Various reputable music and entertainment publications, such as Rolling Stone and NME, also acknowledge \"Thriller\" as the first music video on MTV. (Sources: https://www.rollingstone.com/music/music-lists/100-greatest-music-videos-66601/michael-jackson-thriller-1983-66684/ and https://www.nme.com/photos/100-greatest-music-videos-ever-1426112)\n",
    "\n",
    "> Evidence against:\n",
    "> 1. There is no credible evidence or sources that dispute the claim that \"Thriller\" was the first music video on MTV.\n",
    "\n",
    "> Assessment: <TRUE>\n",
    "\n",
    "Not only is the evidence incorrect, the urls either give a 404 error or are not about Thriller.\n",
    "\n",
    "### Metrics\n",
    "\n",
    "We next compute some metrics.\n",
    "\n",
    "#### Precision, recall, F1\n",
    "\n",
    "Below are computations for these metrics.  Asking for some evidence seems worse than asking for none, but asking for more evidence gives better results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "ec6bbf9e-b0cf-46f6-b91d-b1848ce7bef6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "import numpy as np\n",
    "\n",
    "def computeMetrics(inFile):\n",
    "    df = pd.read_csv(inFile, index_col=None, dtype=str)\n",
    "    preds = df['gptAnswer'].tolist()\n",
    "    trues = df['trueAnswer'].tolist()\n",
    "    labels = [True, False, 'Unknown']\n",
    "    p,r,f,s = precision_recall_fscore_support(trues, preds, labels=labels, average=None,\n",
    "                                             zero_division=np.nan)\n",
    "    prf = np.array([p[:2], r[:2], f[:2]])\n",
    "    print('Precision, recall, F1 for True, False')\n",
    "    print(prf)\n",
    "    cm = confusion_matrix(trues, preds, labels=labels)\n",
    "    print('Confusion matrix for True, False, Unknown')\n",
    "    print(cm)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "5cbb3edb-78b5-4899-a137-28da1d21cbd4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision, recall, F1 for True, False\n",
      "[[0.67821782 0.73571429]\n",
      " [0.685      0.50738916]\n",
      " [0.68159204 0.60058309]]\n",
      "Confusion matrix for True, False, Unknown\n",
      "[[137  37  26]\n",
      " [ 65 103  35]\n",
      " [  0   0   0]]\n"
     ]
    }
   ],
   "source": [
    "# Version not asking for evidence\n",
    "\n",
    "computeMetrics(os.path.join(RESULTDIR, 'triviaQA_dev_noEvidence.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "4dc8dc13-815c-45be-8077-7f35ba32d4df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision, recall, F1 for True, False\n",
      "[[0.89010989 0.73099415]\n",
      " [0.40703518 0.61576355]\n",
      " [0.55862069 0.6684492 ]]\n",
      "Confusion matrix for True, False, Unknown\n",
      "[[ 81  46  72]\n",
      " [ 10 125  68]\n",
      " [  0   0   0]]\n"
     ]
    }
   ],
   "source": [
    "# asking for evidence\n",
    "computeMetrics(os.path.join(RESULTDIR, 'triviaQA_dev_withEvidence.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "b9bb10e6-4a85-40df-af6b-ce9ba86b868a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision, recall, F1 for True, False\n",
      "[[0.79617834 0.76190476]\n",
      " [0.625      0.70935961]\n",
      " [0.70028011 0.73469388]]\n",
      "Confusion matrix for True, False, Unknown\n",
      "[[125  45  30]\n",
      " [ 32 144  27]\n",
      " [  0   0   0]]\n"
     ]
    }
   ],
   "source": [
    "# asking for 4 pieces of evidence\n",
    "\n",
    "computeMetrics(os.path.join(RESULTDIR, 'triviaQA_dev_4Pieces.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b48543d-3aac-42f1-949b-041786847ff3",
   "metadata": {},
   "outputs": [],
   "source": [
    "I next compare pairs of versions using a confusion matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "51950c1e-3e5b-4f6f-b471-3be4727e96fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compare versions\n",
    "\n",
    "def compareVersions(leftFile, rightFile, labels):\n",
    "    dfLeft = pd.read_csv(leftFile, index_col=None, dtype=str)\n",
    "    dfRight = pd.read_csv(rightFile, index_col=None, dtype=str)\n",
    "    dfM = pd.merge(dfLeft, dfRight, how='inner', on='question', suffixes=labels)\n",
    "    #print(dfM)\n",
    "    cm = confusion_matrix(dfM[f\"gptAnswer{labels[0]}\"], \n",
    "                          dfM[f\"gptAnswer{labels[1]}\"], \n",
    "                          labels=['True', 'False', 'Unknown'])\n",
    "    print(f\"Confusion matrix for {labels[0]} X {labels[1]}. (no_evidence is rows)\")\n",
    "    print(cm)\n",
    "    cm = confusion_matrix(dfM[dfM[f\"trueAnswer{labels[0]}\"] == 'True'][f\"gptAnswer{labels[0]}\"], \n",
    "                          dfM[dfM[f\"trueAnswer{labels[0]}\"] == 'True'][f\"gptAnswer{labels[1]}\"], \n",
    "                          labels=['True', 'False', 'Unknown'])\n",
    "    print(f\"\\nConfusion matrix for only true statements, {labels[0]} X {labels[1]}\")\n",
    "    print(cm)\n",
    "    cm = confusion_matrix(dfM[dfM[f\"trueAnswer{labels[0]}\"] == 'False'][f\"gptAnswer{labels[0]}\"], \n",
    "                          dfM[dfM[f\"trueAnswer{labels[0]}\"] == 'False'][f\"gptAnswer{labels[1]}\"], \n",
    "                          labels=['True', 'False', 'Unknown'])\n",
    "    print(f\"\\nConfusion matrix for only false statements, {labels[0]} X {labels[1]}\")\n",
    "    print(cm)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "11842cdf-b24d-40ed-b6da-cd49e48fcafc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion matrix for labels[0] X labels[1]. (no_evidence is rows)\n",
      "[[81 54 67]\n",
      " [ 5 96 38]\n",
      " [ 5 21 35]]\n",
      "\n",
      "Confusion matrix for only true statements, _noEvidence X _withEvidence\n",
      "[[72 26 39]\n",
      " [ 4 17 15]\n",
      " [ 5  3 18]]\n",
      "\n",
      "Confusion matrix for only false statements, _noEvidence X _withEvidence\n",
      "[[ 9 28 28]\n",
      " [ 1 79 23]\n",
      " [ 0 18 17]]\n"
     ]
    }
   ],
   "source": [
    "# compare noEvidence with withEvidence\n",
    "compareVersions(os.path.join(RESULTDIR, 'triviaQA_dev_noEvidence.csv'),\n",
    "               os.path.join(RESULTDIR, 'triviaQA_dev_withEvidence.csv'),\n",
    "              ['_noEvidence', '_withEvidence'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbb59e1a-bd80-4555-9092-5ed50bdce043",
   "metadata": {},
   "source": [
    "## Conclusions\n",
    "\n",
    "Factchecking is meant to be a way to verify whether a statement is true. In addition providing evidence is meant to help people gain confidence in the judgement of the system.\n",
    "\n",
    "1. the metrics are relatively low\n",
    "2. chatGPT makes up sources and arguments, obliterating any confidence this approach was supposed ot provide to people.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
